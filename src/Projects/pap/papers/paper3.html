<!DOCTYPE html><html lang="en" dir="ltr">
    <head>
        <meta charset="utf-8">
        <title>Abstraction and Analogy-Making in Artificial Intelligence</title>
        <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
        <link rel="stylesheet" href="a11y.css">
        <link rel="stylesheet" href="style.css">
        <link href='https://fonts.googleapis.com/css?family=IBM Plex Sans Condensed' rel='stylesheet'>
    </head>
    <body>

        

<div class="sr-only">
    <p>Go To:</p>
    <a href="#paper-title">Paper Title</a>
    <a href="#paper-authors">Paper Authors</a>
    <a href="#paper-toc">Table Of Contents</a>
    <a href="#paper-abstract">Abstract</a>
    <a href="#paper-references">References</a>
</div>
<a href="/projects/papyrus/pap/index.html" class="close2" style="text-decoration: none; cursor: pointer;" aria-label="Return to project Papyrus index page">⇦</a>


<main class="app">

<!-- <div>
    <p><a href="/about">Learn more about this prototype</a></p>
<div> -->

<article class="paper">
    <div class="paper__head">
        
 
        <p> </p>
        <h1 class="paper__title" id="paper-title">Abstraction and Analogy-Making in Artificial Intelligence</h1>

        <h2 class="sr-only" id="paper-authors">Authors</h2>
        <ul class="paper__meta">
             
                <li class="paper__meta-item">Melanie 
                    
                Mitchell</li> 
            
            
            
            
            
        </ul>


    </div>
    <a href="#" class="nav-toggle-btn">
        <img src="/projects/papyrus/pap/papers/img/toc.webp" alt="Toggle Navigation">
    </a>
    <nav class="paper__nav">
        <div class="paper__toc card">
            <!-- TOC -->
            <h2 id="paper-toc" class="toc__header">Abstraction and Analogy-Making in Artificial Intelligence</h2>
            <ul>
                
                    <li><a href="#paper-abstract">Abstract</a></li>
                
                
                <li>
                    <a href="#section-body-1">Introduction</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-2">Abstraction And Analogy-Making In Intelligence</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-3">Symbolic Methods</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-3-1">
                                
                                Figure 1...
                                
                                
                            </a>
                        </li>
                    
                        
                        <li>
                            <a href="#fig-3-2">
                                
                                Figure 2...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-4">Separation Of Representation And Mapping Processes.</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-4-1">
                                
                                Figure 3...
                                
                                
                            </a>
                        </li>
                    
                        
                        <li>
                            <a href="#fig-4-2">
                                
                                Figure 4...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-5">Deep Learning Approaches</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-5-1">
                                
                                Figure 5...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-6">Probabilistic Program Induction</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-6-1">
                                
                                Figure 6...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-7">Solving Bongard Problems</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-7-1">
                                
                                Figure 7...
                                
                                
                            </a>
                        </li>
                    
                        
                        <li>
                            <a href="#fig-7-2">
                                
                                Figure 8...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-8">P(R|E, G) ∝ P(E|R)P(R|G).</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-9">For Details).</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-10">Summary Of Program-Induction Approaches</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-11">Abstraction And Reasoning Corpus</a>
                    
                    
                    <ul>
                    
                    
                        
                        <li>
                            <a href="#fig-11-1">
                                
                                Figure 9...
                                
                                
                            </a>
                        </li>
                    
                    </ul>
                    
                </li>
                
                <li>
                    <a href="#section-body-12">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-13">Conclusion</a>
                    
                    
                </li>
                
                <li>
                    <a href="#section-body-14">SECTION</a>
                    
                    
                </li>
                
                <li><a href="#paper-references">References</a></li>
            </ul>
        </div>
    </nav>

    <div class="paper__text">
        <!--Only show abstract if exists-->
        
            <h2 id="paper-abstract" tabindex="0"> Abstract </h2>
            <p tabindex="0"> Conceptual abstraction and analogy-making are key abilities underlying humans&#39; abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.</p>
        

        <!-- Body text -->
        
            <h2 id="section-body-1" tabindex="0">Introduction </h2>
            
            
            
                
                    <p tabindex="0">Without concepts there can be no thought, and without analogies there can be no concepts. </p>
                
            
                
                    <p tabindex="0">-Hofstadter and Sander <a href="#BIBREF0"  id="BIBREF0-Introduction-ref">1</a> In their 1955 proposal for the Dartmouth summer artificial intelligence (AI) project, John McCarthy and colleagues wrote, "An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves." 2 Now, nearly seven decades later, all these research topics remain open and actively investigated in the AI community. While AI has made dramatic progress over the last decade in areas such as computer vision, natural language processing, and robotics, current AI systems almost entirely lack the ability to form humanlike concepts and abstractions. </p>
                
            
                
                    <p tabindex="0">For example, while today's computer vision systems can recognize perceptual categories of objects, such as labeling a photo of the Golden Gate as "a bridge," these systems lack the rich conceptual knowledge humans have about these objectsknowledge that enables robust recognition of such objects in a huge variety of contexts. Moreover, humans are able to form abstractions and apply them to novel situations in ways that elude even the best of today's machines. Continuing with the "bridge" example, humans can easily understand extended and metaphorical notions such as "water bridges," "ant bridges," "bridging one's fingers," "bridge of one's nose," "the bridge of a song," "bridging the gender gap," "a bridge loan," "burning one's bridges," "water under the bridge," and so on. Indeed, for humans, any perceptual category such as bridge is understood via the rich conceptual structure underlying it. This conceptual structure makes it easy for humans to answer commonsense questions like "What would happen if you drove across a raised drawbridge?" or "What is on each side of a bridge across the gender gap?" Moreover, conceptual structures in the mind make it easy for humans to generate "bridges" at different levels of abstraction; for example, imagine yourself forming a bridge from a couch to a coffee table with your leg, or forming a bridge between two notes on your piano with other notes, or bridging differences with your spouse via a conversation. </p>
                
            
                
                    <p tabindex="0">Most, if not all, human concepts can be abstracted in this way, via analogy (for example, as an interesting exercise, consider the possible abstractions of everyday concepts such as mirror, shadow, ceiling, driving, and sinking). Hofstadter goes so far as to define concepts by this property: "a concept is a package of analogies." <a href="#BIBREF2"  id="BIBREF2-Introduction-ref">3</a> Humans' abilities for abstraction and analogy are at the root of many of our most important cognitive capacities, and the lack of these abilities is at least partly responsible for current AI's brittleness and its difficulty in adapting knowledge and representations to new situations. Today's state-of-the-art AI systems often struggle in transferring what they have learned to situations outside their training regimes, <a href="#BIBREF3"  id="BIBREF3-Introduction-ref">4</a> they make unexpected and unhumanlike errors, <a href="#BIBREF4"  id="BIBREF4-Introduction-ref">5</a> and they are vulnerable to "adversarial examples" in a very unhumanlike way. <a href="#BIBREF5"  id="BIBREF5-Introduction-ref">6,</a> <a href="#BIBREF6"  id="BIBREF6-Introduction-ref">7</a> Concepts, abstraction, and analogy are foundational areas of study in cognitive psychology. Psychological theories of concepts have focused on ideas such as core knowledge, <a href="#BIBREF7"  id="BIBREF7-Introduction-ref">8,</a> <a href="#BIBREF8"  id="BIBREF8-Introduction-ref">9</a> exemplars and prototypes, <a href="#BIBREF9"  id="BIBREF9-Introduction-ref">10,</a> <a href="#BIBREF10"  id="BIBREF10-Introduction-ref">11</a> the "theory theory" of concepts, <a href="#BIBREF11"  id="BIBREF11-Introduction-ref">12</a> perceptual simulations, <a href="#BIBREF12"  id="BIBREF12-Introduction-ref">13</a> experiencebased metaphors, <a href="#BIBREF13"  id="BIBREF13-Introduction-ref">14</a> and stochastic functions in a probabilistic "language of thought." <a href="#BIBREF14"  id="BIBREF14-Introduction-ref">15</a> Some cognitive scientists have postulated that all concepts, even the most abstract ones, correspond to mental models of the world-grounded in perceptionthat can be simulated to yield predictions and counterfactuals. <a href="#BIBREF12"  id="BIBREF12-Introduction-ref">13,</a> <a href="#BIBREF15"  id="BIBREF15-Introduction-ref">[16]</a> <a href="#BIBREF16"  id="BIBREF16-Introduction-ref">[17]</a> <a href="#BIBREF17"  id="BIBREF17-Introduction-ref">[18]</a> <a href="#BIBREF18"  id="BIBREF18-Introduction-ref">[19]</a> Others have argued that concept formation, abstraction, and adaptation are all undergirded by processes of analogy-the act of perceiving essential similarities between entities or situations. <a href="#BIBREF2"  id="BIBREF2-Introduction-ref">3,</a> <a href="#BIBREF20"  id="BIBREF20-Introduction-ref">20,</a> <a href="#BIBREF21"  id="BIBREF21-Introduction-ref">21</a> Understanding what concepts arehow they are formed, how they can be abstracted and flexibly used in new situations, and how they can be composed to produce new concepts-is not only key to a deeper understanding of intelligence, but will be essential for engineering nonbrittle AI systems, ones that can abstract, robustly generalize, resist adversarial inputs, and adapt what they have learned to diverse domains and modalities. </p>
                
            
                
                    <p tabindex="0">The purpose of this paper is to review selected AI research-both old and very recent-on abstraction and analogy-making, to make sense of what this research has yielded for the broader problem of creating machines with these general abilities, and to make some recommendations for the path forward in this field. </p>
                
            
                
                    <p tabindex="0">The remainder of the paper is organized as follows. The next section gives a brief discussion of the role of abstraction and analogy-making in human intelligence, and the need for such abilities in AI. The following sections review several approaches in the AI literature to capturing abstraction and analogy-making abilities, particularly using idealized domains. The paper concludes with a discussion that appraises the domains and methods discussed here and proposes several steps for making generalizable progress on these issues. </p>
                
            
        
            <h2 id="section-body-2" tabindex="0">Abstraction And Analogy-Making In Intelligence </h2>
            
            
            
                
                    <p tabindex="0">One makes a conceptual abstraction when one extends that concept to novel situations, including ones that are removed from perceptual entities, as in the examples of "bridge" I gave earlier. The process of abstraction is driven by analogy, in which one mentally maps the essence of one situation to a different situation (e.g., mapping one's concept of a bridge across a river to a bridge across the gender gap). We make an analogy every time our current situation reminds us of a past one, or when we respond to a friend's story with "the same thing happened to me," even though "the same thing" can be superficially very different. In fact, the primary way we humans make sense of novel situations is by making analogies to situations we have previously experienced. Analogies underlie our abilities to flexibly recognize new instances of visual concepts, such as "a ski race" or "a protest march." Analogies drive our conceptualizations of wholly new situations, such as the novel coronavirus pandemic that erupted in early 2020, in terms of things we know something about-the pandemic has been variously described as a fire, a tsunami, a tornado, a volcano, "another Katrina," or a war. Recognizing abstract styles of art or music is also a feat of analogy-making. Many scientific insights are based on analogies, such as Darwin's realization that biological competition is analogous to economic competition <a href="#BIBREF22"  id="BIBREF22-Abstraction And Analogy-Making In Intelligence-ref">22</a> and Von Neumann's analogies between the computer and the brain. <a href="#BIBREF23"  id="BIBREF23-Abstraction And Analogy-Making In Intelligence-ref">23</a> (These are just a few examples; see Ref. 1 for many more examples at all levels of cognition.) </p>
                
            
                
                    <p tabindex="0">These examples illustrate two important facts: (1) analogy-making is not a rare and exalted form of reasoning, but rather a constant, ubiquitous, and lifelong mode of thinking; (2) analogy is a key aspect not only of reasoning but also of flexible categorization, concept formation, abstraction, and counterfactual inference. <a href="#BIBREF0"  id="BIBREF0-Abstraction And Analogy-Making In Intelligence-ref">1,</a> <a href="#BIBREF2"  id="BIBREF2-Abstraction And Analogy-Making In Intelligence-ref">3,</a> <a href="#BIBREF20"  id="BIBREF20-Abstraction And Analogy-Making In Intelligence-ref">20,</a> <a href="#BIBREF24"  id="BIBREF24-Abstraction And Analogy-Making In Intelligence-ref">24</a> In short, analogy is a central mechanism for unlocking meaning from perception. (See Ref. <a href="#BIBREF25"  id="BIBREF25-Abstraction And Analogy-Making In Intelligence-ref">25</a> for a broad discussion of analogy and its relation to other mechanisms of cognition and reasoning.) </p>
                
            
                
                    <p tabindex="0">Analogy-making gives rise to human abilities lacking in even the best current-day AI systems. Many researchers have pointed out the need for AI systems that are more robust and general-that is, AI systems that can perform sophisticated transfer learning or "low-shot" learning, that robustly can figure out how to make sense of novel situations, and that can form and use abstract concepts. Given this, it seems that analogy-making, and its role in abstraction, is an understudied area in AI that deserves more attention. </p>
                
            
                
                    <p tabindex="0">In the following sections I review several selected approaches to studying abstraction and analogymaking in AI systems, ranging from older symbolic or hybrid approaches such as the structure-mapping approach of Gentner et al. and the "active symbol" approach of Hofstadter et al., to more recent methods that employ deep neural networks and probabilistic program induction. This is not meant to be an exhaustive survey of this topic, but rather a review of some of the more prominent approaches, ones that I will analyze in the Discussion section. Note that I will focus on concepts and analogies that involve multipart situations, rather than the simpler single-relation "proportional" analogies such as man:woman :: king:?" <a href="#BIBREF26"  id="BIBREF26-Abstraction And Analogy-Making In Intelligence-ref">26,</a> <a href="#BIBREF27"  id="BIBREF27-Abstraction And Analogy-Making In Intelligence-ref">27</a> While the term "analogy" often brings to mind proportional analogies like these, the range of human analogy-making is far more interesting, rich, and ubiquitous. </p>
                
            
        
            <h2 id="section-body-3" tabindex="0">Symbolic Methods </h2>
            
            
            
                
                    <p tabindex="0">Symbolic approaches to abstraction and analogy typically represent input as structured sets of logiclike statements, with concepts expressed as naturallanguage words or phrases. Early examples include Evans' geometric-analogy solver, <a href="#BIBREF28"  id="BIBREF28-Symbolic Methods-ref">28</a> Winston's frame-based system for analogy-making between stories, <a href="#BIBREF29"  id="BIBREF29-Symbolic Methods-ref">29</a> and Falkenhainer et al.'s structuremapping engine (SME). <a href="#BIBREF30"  id="BIBREF30-Symbolic Methods-ref">30</a> In this section I will describe SME, as well as the active symbol architec-ture of Hofstadter et al., which combines symbolic and subsymbolic elements. </p>
                
            
                
                    <p tabindex="0">The structure-mapping engine The SME <a href="#BIBREF30"  id="BIBREF30-Symbolic Methods-ref">30</a> is based on Gentner's structuremapping theory of human analogy-making. <a href="#BIBREF31"  id="BIBREF31-Symbolic Methods-ref">31</a> In Gentner's theory, analogical mapping of one entity to another depends only on "syntactic properties of the knowledge representation [describing the entities], and not on the specific content of the domains." <a href="#BIBREF31"  id="BIBREF31-Symbolic Methods-ref">31</a> Furthermore, according to the theory, mappings are primarily made between relations rather than object attributes, and analogies are driven primarily by mappings between higher order relations (the "systematicity" principle). </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-3-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig1.webp" alt="Figure 1" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 1. An example of SME’s input in the form of logical predicates. Adapted from Ref. 30.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">SME's input consists of descriptions of two entities or situations, a base and a target, where each description consists of a set of logical predicates. In <a href="#FIGREF0"  id="FIGREF0-Symbolic Methods-ref">Figure 1</a> , adapted from one of Falkenhainer et al.'s illustrative examples, the base gives predicates about the solar system and the target gives predicates about the Rutherford atom. </p>
                
            
                
                    <p tabindex="0">The descriptions in <a href="#FIGREF0"  id="FIGREF0-Symbolic Methods-ref">Figure 1</a> are logical statements represented as trees. The statements include entities (e.g., planet), attributes (e.g., yellow), firstorder relations (e.g., revolves-around), and higher order relations (e.g., causes). SME's job is to create a coherent mapping from the base to the target. The program uses a set of heuristics to make candidate pairings between elements of the base and target. Examples of such heuristics are: "If two relations have the same name, then pair them"; "If two objects play the same role in two already paired relations (i.e., are arguments in the same position), then pair them." The program then scores each of the pairings, based on factors such as having the same name or being part of "systematic structures," that is, more deeply nested predicates rather than isolated relations. </p>
                
            
                
                    <p tabindex="0">Once all plausible pairings have been made, the program performs a heuristic search and merging procedure to construct a "global mapping" between the base and target. The global match is given a score based on the individual pairings it is made up of, the inferences it suggests, and its degree of systematicity (relations that are part of a coherent interconnected system are preferentially mapped over relatively isolated relations). </p>
                
            
                
                    <p tabindex="0">It is important to note that SME is considered to be a model solely of the mapping process of analogy-making; it assumes that the situations to be mapped have already been represented in predicate form. SME has often been used as a mapping module in a system that has other modules for representation, retrieval, and inference. <a href="#BIBREF32"  id="BIBREF32-Symbolic Methods-ref">[32]</a> <a href="#BIBREF33"  id="BIBREF33-Symbolic Methods-ref">[33]</a> <a href="#BIBREF34"  id="BIBREF34-Symbolic Methods-ref">[34]</a> One notable example of such a system is the work of Lovett et al. <a href="#BIBREF35"  id="BIBREF35-Symbolic Methods-ref">35</a> on solving Raven's progressive matrices (RPMs). I will use this work as an illustrative example, since RPMs have more recently been studied extensively in the deep learning community as a domain for studying abstraction and analogy. </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-3-2" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig2.webp" alt="Figure 2" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 2. A sample RPM problem.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">RPMs, originally created by John Raven in the 1930s as way to assess "fluid intelligence," have long been given to both children and adults as nonverbal intelligence tests. <a href="#FIGREF1"  id="FIGREF1-Symbolic Methods-ref">Figure 2</a> gives a sample problem. The 3 × 3 grid on the left shows patterns of change along the rows and columns; the challenge is to decide which figure from the eight choices on the right fits the blank square. Here, the answer is 5. This example is one of the easier RPM problems; often they involve more than two shapes per square and changes in multiple attributes (shape, color, position, number, etc.) </p>
                
            
                
                    <p tabindex="0">Lovett et al. <a href="#BIBREF35"  id="BIBREF35-Symbolic Methods-ref">35</a> used SME as one component of a system to solve RPM problems. The input to SME was a set of predicate-logic descriptions of each square, generated by the CogSketch system. <a href="#BIBREF36"  id="BIBREF36-Symbolic Methods-ref">36</a> To form these descriptions, CogSketch is programmed with a repertoire of possible object attributes (e.g., size, position, and degree of symmetry) and object relations (e.g., inside/outside, intersection, and rotation) that it looks for in each box in a given RPM problem. The system can also create qualitative descriptions of the edges making up objects (e.g., relations such as relative orientation, relative length, etc.) as well as descriptions of object groupings based on proximity and similarity. </p>
                
            
                
                    <p tabindex="0">These predicate-logic representations are input to SME, which returns the highest scoring mapping between descriptions of pairs of images in a row. This mapping is used by the system to determine a higher-level description of the pattern of change across each of the top two rows, from an existing vocabulary of possible changes between corresponding objects (e.g., identity, deformation, shape change, and addition/removal). </p>
                
            
                
                    <p tabindex="0">These higher-level descriptions of the top two rows are themselves mapped by SME to produce a description of more general pattern. SME scores each of the eight possible answers to see which one best completes the third row according to this pattern. (This brief description leaves out some additional details, such as special-purpose operations for certain kinds of problems.) </p>
                
            
                
                    <p tabindex="0">Lovett et al. ran this system on the standard progressive matrices test. Their system was able to solve 56 out of the 60 problems. </p>
                
            
                
                    <p tabindex="0">Lovett et al.'s experiments provide an evaluation of SME as part of a larger system for making analogies in the context of RPMs. I will say more about the RPM domain in the next section. SME captures an important aspect of analogymaking: people tend to prefer systematic analogies involving more abstract concepts rather than less systematic mappings involving more superficial concepts. <a href="#BIBREF31"  id="BIBREF31-Symbolic Methods-ref">31</a> However, I believe that the SME approach is limited in its ability to capture analogymaking more generally, for three main reasons, as I describe next. Focus on syntax rather than semantics. The goal of SME is to give a domain-independent account of analogy-making; thus, true to its name, SME focuses on mapping the structure or syntax of its input representations rather than domain-specific semantics. However, this reliance on syntactic structure requires that representations of situations be cleanly partitioned into objects, attributes, functions, first-order relations, second-order relations, and so on. The problem is that humans' mental representations of real-world situations are generally not so rigidly categorized. Building on the "bridge" example from the introduction, consider a mapping between the Golden Gate bridge and President Joe Biden's statement that he is a "bridge" to a future generation of leaders. <a href="#BIBREF37"  id="BIBREF37-Symbolic Methods-ref">37</a> Should "bridge" be represented as an attribute of the object "Golden Gate" (bridge(GoldenGate)), an object itself, with its own attributes (e.g., golden(Bridge)), or a relation between the areas on either side: bridge(GoldenGate, San Francisco, MarinCounty)? In the latter case, if the Joe Biden "bridge" is represented only as a two-argument relation (bridge(JoeBiden, Future-Generation)), it would not be paired with the three-argument Golden Gate relationship. Or what if "future generation" is not a unitary object but an attribute (e.g., FutureGeneration(Leaders))? Then it could not be matched with, say, the "object" Mar-inCounty. These examples illustrate that real-world situations are not easily reducible to unambiguous predicates that can be mapped via syntax alone. </p>
                
            
                
                    <p tabindex="0">The distinctions between "object," "attribute," and "relation," not to mention the order of the relation, depend heavily on context, and people (if they assign such categories at all) have to use them very flexibly, allowing initial classifications to slide if necessary at the drop of a hat. However, a major tenet of SME is that the mapping process is performed on existing representations. This naturally leads into the second issue, the separation of representation-building and mapping. </p>
                
            
        
            <h2 id="section-body-4" tabindex="0">Separation Of Representation And Mapping Processes. </h2>
            
            
            
                
                    <p tabindex="0">The SME approach separates "representation-building" and "mapping" into two distinct and independent phases in making an analogy. The base and target entities are rendered (by a human or another program) into predicatelogic descriptions, which is then given to SME to construct mappings between these descriptions. </p>
                
            
                
                    <p tabindex="0">Some versions of SME enable a limited "rerepresentation" of the base or target in response to low-scoring mappings. <a href="#BIBREF38"  id="BIBREF38-Separation Of Representation And Mapping Processes.-ref">38</a> Such rerepresentations might modify certain predicates (e.g., factoring them into "subpredicates") but in the examples given in the literature, SME almost entirely relies on another module to build representations before mapping takes place. </p>
                
            
                
                    <p tabindex="0">In the next section I will discuss a different view, arguing that in human analogy-making, the process of building representations is inextricably intertwined with the mapping process, and that we should adopt this principle in building AI systems that make analogies. This argument was also made in detail in several previous publications <a href="#BIBREF39"  id="BIBREF39-Separation Of Representation And Mapping Processes.-ref">[39]</a> <a href="#BIBREF40"  id="BIBREF40-Separation Of Representation And Mapping Processes.-ref">[40]</a> <a href="#BIBREF41"  id="BIBREF41-Separation Of Representation And Mapping Processes.-ref">[41]</a> and counterarguments were given by the SME authors. <a href="#BIBREF38"  id="BIBREF38-Separation Of Representation And Mapping Processes.-ref">38</a> Semiexhaustive search. Finally, SME relies on semiexhaustive search over matchings. The program considers matches between all "plausible pairings" to create multiple global matches. While this is not a problem if representations have been boiled down to a relatively small number of predicates, it is not clear that this semiexhaustive method will scale well in general. </p>
                
            
                
                    <p tabindex="0">In short, the structure-mapping theory makes some very useful points about what features appealing analogies tend to have, but in dealing only with the mapping process while leaving aside the problem of how situations become understood and how this process of interpretation interacts with the mapping process, it leaves out some of the most important aspects of how analogies are made. </p>
                
            
                
                    <p tabindex="0">In addition to symbolic approaches such as SME, several hybrid symbolic-connectionist approaches have also been explored, such as the ACME and LISA systems, <a href="#BIBREF42"  id="BIBREF42-Separation Of Representation And Mapping Processes.-ref">42,</a> <a href="#BIBREF43"  id="BIBREF43-Separation Of Representation And Mapping Processes.-ref">43</a> neural networks to perform mapping between symbolic representations, <a href="#BIBREF44"  id="BIBREF44-Separation Of Representation And Mapping Processes.-ref">44</a> as well as systems based on cognitive models of memory. <a href="#BIBREF45"  id="BIBREF45-Separation Of Representation And Mapping Processes.-ref">45</a> Active symbol architecture In the 1980s, Hofstadter designed a general architecture for abstraction and analogy-making that I will call the "active symbol architecture," based in part on Hofstadter's notion of active symbols in the brain: "active elements [groups of neurons] which can store information and transmit it and receive it from other active elements" <a href="#BIBREF46"  id="BIBREF46-Separation Of Representation And Mapping Processes.-ref">46</a> -and in part on inspiration from information processing in other complex systems such as ant colonies and cellular metabolism. </p>
                
            
                
                    <p tabindex="0">The active symbol architecture was the basis for several AI programs exploring abstraction and analogy-making, many of which were described in Ref. <a href="#BIBREF47"  id="BIBREF47-Separation Of Representation And Mapping Processes.-ref">47</a> . Here, I will focus on Hofstadter and Mitchell's Copycat program. <a href="#BIBREF41"  id="BIBREF41-Separation Of Representation And Mapping Processes.-ref">41,</a> <a href="#BIBREF48"  id="BIBREF48-Separation Of Representation And Mapping Processes.-ref">48</a> The name "Copycat" is a humorous reference to the idea that the act of making an analogy is akin to being a "copycat"that is, understanding one situation and "doing the same thing" in a different situation. A key idea of the Copycat program is that analogy-making should be modeled as a process of abstract perception. Like sensory perception, analogy-making is a process in which one's prior concepts are activated by a situation, either perceived via the senses or in the mind's eye; those activated concepts adapt to the situation at hand and feed back to affect how that situation is perceived. </p>
                
            
                
                    <p tabindex="0">Hofstadter and Mitchell developed and tested Copycat using the domain of letter-string analogy problems, created by Hofstadter. <a href="#BIBREF49"  id="BIBREF49-Separation Of Representation And Mapping Processes.-ref">49</a> While these analogy problems are idealized "toy" problems, they are, similar to Raven's matrices, designed to capture something of the essence of real-world abstraction and analogy-making. Each string is an idealized "situation" containing objects (e.g., letters or groupings of letters), relations among objects, events (a change from the first to second string), and a requirement for abstraction via what Hofstadter termed conceptual slippages <a href="#BIBREF49"  id="BIBREF49-Separation Of Representation And Mapping Processes.-ref">49</a> (e.g., the role of "letter" in one situation is played by "group" in another situation, or the role of "successsor" in one situation is played by "predecessor" in another situation). </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-4-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig3.webp" alt="Figure 3" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 3. (A) Illustration of part of Copycat’s concept network. (B) Illustration of Copycat’s workspace, during a run of the program.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">In the Copycat system, the process of analogical mapping between two situations (here, letter strings) is interleaved with the process of building representations of those situations, with continual feedback between these processes. This is achieved via four interacting components: a concept network, which contains the system's prior knowledge in symbolic form; a workspace, which serves as a working memory in which representation of and mappings between the input situations takes place; a set of perceptual agents, which-competitively and cooperatively-attempt to adapt the system's prior knowledge to the input situations over a series of time steps; and a temperature variable, which measures the quality and coherence of the system's representations and mappings at a given time, and which feeds back to control the degree of randomness of the perceptual agents. When the system is far from a solution, the temperature is high, and the perceptual agents' actions are more random; as the system zeroes in on a coherent solution, the temperature falls, and the perceptual agents are more deterministic. <a href="#FIGREF2"  id="FIGREF2-Separation Of Representation And Mapping Processes.-ref">Figure 3A</a> illustrates part of the program's concept network, which contains the program's prior (symbolic) knowledge about the letter-string domain, corresponding to long-term memory. The concept network models a symbolic "semantic space," in which concepts are nodes (ellipses) and links (lines) between concepts represent semantic distance, which can change with context during a run of the program. A concept (e.g., letter group) is activated when instances of that concept are discovered in the workspace, and in turn, activated concepts (the program's "active symbols") trigger perceptual agents that attempt to discover additional instances. Activation can also spread between conceptual neighbors. Activation decays over time if not reinforced. <a href="#FIGREF2"  id="FIGREF2-Separation Of Representation And Mapping Processes.-ref">Figure 3B</a> illustrates the program's workspace, a short-term memory, inspired by blackboard systems, <a href="#BIBREF50"  id="BIBREF50-Separation Of Representation And Mapping Processes.-ref">50</a> in which perceptual agents construct (and sometimes destroy) structures (relations, groupings, correspondences, and rules) that form the program's current representation of the input situations and the analogy between them, at any given time during a run. Dashed lines or arcs represent structures with low confidence; solid lines or arcs represent structures with high confidence; the confidence of a structure can change during the run and structures can be destroyed depending on their confidence. A temperature variable (represented by the thermometer in the bottom right of the workspace) measures the quality of the current structures and feeds back to affect the randomness of the perceptual agents. <a href="#FIGREF4"  id="FIGREF4-Separation Of Representation And Mapping Processes.-ref">Figure 4</a> gives the state of the workspace at selected time steps during a run of the program, illustrating how the program constructs representations of, and analogies between, its input situations. The workspace serves as a global blackboard on which agents explore, build, and destroy possible structures. The actions of agents are probabilistic, and depend on the current state of the workspace, concept network, and temperature. Perceived correspondences between objects in different situations (here, letters and letter groups) lead to conceptual slippages (e.g., letter slips to letter group) that give rise to a coherent analogy. Details of Copycat's operations are described in Ref. <a href="#BIBREF48"  id="BIBREF48-Separation Of Representation And Mapping Processes.-ref">48</a> . </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-4-2" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig4.webp" alt="Figure 4" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 4. State of the workspace at six different time steps during a run of Copycat. Adapted from Ref. 48.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">In summary, the Copycat program is an example of Hofstadter's active symbol architecture, in which symbolic concepts become activated via bottomup perceptions, spread activation to semantically related neighbors, and influence perception in a top-down manner by triggering probabilistic perceptual agents to find instances of the associated concepts in a blackboard-like workspace. In this way, processing in the system consists of a continual interplay between bottom-up and top-down processes. A temperature variable controls the degree of randomness in the system and, in turn, is dynamically determined by the quality of perceptual structures constructed by the system. Coherent representations of input situations, and analogies between them, result from the perceptual structures constructed by these probabilistic agents. A central idea underlying the active symbol architecture is that, in analogy-making, the mapping process cannot be separated from the representation-building process-these must be interleaved. This is a central point of disagreement with the SME approach described in the previous section (see also Ref. 39). </p>
                
            
                
                    <p tabindex="0">As described in Ref. <a href="#BIBREF41"  id="BIBREF41-Separation Of Representation And Mapping Processes.-ref">41</a> , Copycat's emergent dynamics show a gradual transition from a largely bottom-up (perception-driven), random, and parallel mode of processing-in which many possible representations of the input are explored-to one that is largely top-down (concept-driven), deterministic, and serial. Copycat does not fit neatly into the traditional AI dichotomy between symbolic and neural systems; rather it incorporates symbolic, subsymbolic, and probabilistic elements. The architecture resonates with several ideas in psychology, psychophysics, and neuroscience, such as the Global Workspace hypothesis of Baars et al., <a href="#BIBREF51"  id="BIBREF51-Separation Of Representation And Mapping Processes.-ref">51,</a> <a href="#BIBREF52"  id="BIBREF52-Separation Of Representation And Mapping Processes.-ref">52</a> in which multiple, parallel, specialist processes compete and cooperate for access to a global workspace, and the proposal that visual cortex areas V1 and V2 work as "'active blackboards' that integrate and sustain the result of computations performed in higher areas." <a href="#BIBREF53"  id="BIBREF53-Separation Of Representation And Mapping Processes.-ref">53,</a> <a href="#BIBREF54"  id="BIBREF54-Separation Of Representation And Mapping Processes.-ref">54</a> Copycat also resonates with the idea of neural "object files" <a href="#BIBREF55"  id="BIBREF55-Separation Of Representation And Mapping Processes.-ref">55</a> -temporary and modifiable perceptual structures, created on the fly in working memory, which interact with a permanent network of concepts. The system's dynamics are also in accord with Treisman's <a href="#BIBREF56"  id="BIBREF56-Separation Of Representation And Mapping Processes.-ref">56</a> notion of perception as a shift from parallel, random, "preattentive" bottomup processing and more deterministic, focused, serial, "attentive" top-down processing. </p>
                
            
                
                    <p tabindex="0">Mitchell and Hofstadter showed how Copycat was able to solve a wide selection of letterstring problems; they also described the program's limitations. <a href="#BIBREF41"  id="BIBREF41-Separation Of Representation And Mapping Processes.-ref">41,</a> <a href="#BIBREF48"  id="BIBREF48-Separation Of Representation And Mapping Processes.-ref">48</a> The Copycat program inspired numerous other active symbol architecture approaches to analogy-making, some of which are described in Ref. <a href="#BIBREF47"  id="BIBREF47-Separation Of Representation And Mapping Processes.-ref">47</a> , as well as approaches to music cognition, <a href="#BIBREF57"  id="BIBREF57-Separation Of Representation And Mapping Processes.-ref">57</a> image recognition, <a href="#BIBREF58"  id="BIBREF58-Separation Of Representation And Mapping Processes.-ref">58</a> and more general cognitive architectures. <a href="#BIBREF51"  id="BIBREF51-Separation Of Representation And Mapping Processes.-ref">51</a> It bears repeating that Copycat was not meant to model analogy-making on letter strings per se. Rather, the program was meant to illustrate-using the letter-string analogy domain-a domainindependent model of high-level perception and analogy. However, the program has several limitations that need to be overcome to make it a more general model of analogy-making. For example, Copycat's concept network was manually constructed, not learned; the program illustrated how to adapt preexisting concepts flexibly to new situations, rather than how to learn new concepts. Moreover, the program was given a "source" and "target" situation to compare rather than having to retrieve a relevant situation from memory. Finally, the program's architecture and parameter tuning were complicated and somewhat ad hoc. Additional research on all of these issues is needed to make active symbol architectures more generally applicable. </p>
                
            
        
            <h2 id="section-body-5" tabindex="0">Deep Learning Approaches </h2>
            
            
            
                
                    <p tabindex="0">At the other end of the spectrum from symbolic approaches are deep learning approaches, in which knowledge is encoded as high-dimensional vectors and reasoning is the manipulation of these representations via numeric operations in a deep neural network. Moreover, in deep learning, knowledge representation, abstraction, and analogy-making abilities must be learned, typically via large training sets. </p>
                
            
                
                    <p tabindex="0">Here, I will not survey the substantial literature on pre-deep-learning connectionist modeling of analogy-making (though see, e.g., Refs. 40 and 59). Probably the best-known result of the (post-2010) deep learning era on analogy are the proportional analogies (e.g., "Man is to woman as king is to?") that can arise from vector arithmetic on word embeddings. <a href="#BIBREF26"  id="BIBREF26-Deep Learning Approaches-ref">26</a> More recently, numerous papers have been published applying deep learning methods to make analogies between words or simple images <a href="#BIBREF27"  id="BIBREF27-Deep Learning Approaches-ref">27,</a> <a href="#BIBREF60"  id="BIBREF60-Deep Learning Approaches-ref">60,</a> <a href="#BIBREF61"  id="BIBREF61-Deep Learning Approaches-ref">61</a> as well as on simplified RPM-like problems. <a href="#BIBREF62"  id="BIBREF62-Deep Learning Approaches-ref">62,</a> <a href="#BIBREF63"  id="BIBREF63-Deep Learning Approaches-ref">63</a> In addition, one group demonstrated a deep neural network that can learn to make mappings on symbolic representations (like the ones in <a href="#FIGREF0"  id="FIGREF0-Deep Learning Approaches-ref">Fig. 1</a> ) that roughly agree with the mappings made by SME. <a href="#BIBREF44"  id="BIBREF44-Deep Learning Approaches-ref">44</a> Attempts to get deep learning systems to learn abstract concepts and create analogies are fascinating as ways to explore what these architectures are capable of learning and what kind of reasoning they are able to do. However, these investigations can be complicated by the amount of data needed to train deep learning systems, as well as by their lack of transparency. In this paper, as an illustrative example of the promise and pitfalls of deep learning approaches to abstraction and analogy, I will describe in detail a series of attempts to use deep neural networks on RPM. This domain has become a popular benchmark in the deep learning community. </p>
                
            
                
                    <p tabindex="0">As I described above, SME was tested on part of a standard 60-problem RPM test. However, 60 problems is orders of magnitude too small a dataset to successfully train and test a deep neural net. In order to apply deep learning to this task, researchers need an automated way of generating a very large number of distinct, well-formed, and challenging RPM problems. </p>
                
            
                
                    <p tabindex="0">In 2015, Wang and Su <a href="#BIBREF64"  id="BIBREF64-Deep Learning Approaches-ref">64</a> proposed an RPM problem generation method, guided by an earlier analysis of RPM problems. <a href="#BIBREF65"  id="BIBREF65-Deep Learning Approaches-ref">65</a> Their method generated a problem by repeatedly sampling from a fixed set of object, attribute, and relation types. Building on this approach, Barrett et al. <a href="#BIBREF66"  id="BIBREF66-Deep Learning Approaches-ref">66</a> proposed a related method to produce a large dataset of RPM problems, which they called procedurally generated matrices (PGMs). Each PGM is a set S of triples, where each triple consists of a specific relation, an object type, and an attribute. The generating process first samples from a fixed set of relation, object, and attribute types to create between one and four triples. The generating process then samples allowed values for the object types and attributes. All other necessary attributes (e.g., number of objects) are sampled from allowed values that do not induce spurious relationships. The PGM S is then rendered into pixels. See Ref. <a href="#BIBREF66"  id="BIBREF66-Deep Learning Approaches-ref">66</a> for more details on this process, though the paper did not give details on how the eight candidate answers were generated. </p>
                
            
                
                    <p tabindex="0">Barrett et al. used this generation method to produce their PGM corpus of Raven-style problems. They split the corpus into 1.2M training problems, 20K validation problems, and 200K test problems. They used these splits to train and test several deep learning methods, including their own novel method, the wild relation network (WReN), which utilized relation network modules. <a href="#BIBREF67"  id="BIBREF67-Deep Learning Approaches-ref">67</a> For each network, the input was the eight matrix panels and the eight candidate-answer panels (i.e., 16 grayscale images). The output was a probability distribution over the eight candidate answers. Each network was trained to maximize the probability of the correct answer for a given input. Random guessing would yield a 12.5% accuracy. Among their many experiments, the authors found that their WReN method achieved the best performance of the various networks, an impressive 63% accuracy on the test problems for the so-called "neutral split" (training and test set can contain problems generated with any triples). Other training/test splits that required extrapolation from a restricted set of triples or attribute values to a different set produced notably poorer performance. Subsequent to the work of Barrett et al., several other groups developed new deep learning based methods that improved state-of-the-art accuracy on the PGM corpus. <a href="#BIBREF68"  id="BIBREF68-Deep Learning Approaches-ref">[68]</a> <a href="#BIBREF69"  id="BIBREF69-Deep Learning Approaches-ref">[69]</a> <a href="#BIBREF70"  id="BIBREF70-Deep Learning Approaches-ref">[70]</a> <a href="#BIBREF71"  id="BIBREF71-Deep Learning Approaches-ref">[71]</a> In a 2019 paper, Zhang et al. <a href="#BIBREF72"  id="BIBREF72-Deep Learning Approaches-ref">72</a> questioned whether the PGM dataset was diverse enough to be a good test of abstract reasoning abilities. They noted that even though the PGM corpus is large (over 1.2 million problems), the PGM generation procedure is limited in the possible kind of problems that can be generated. Zhang et al. noted that "PGM's gigantic size and limited diversity might disguise model fitting as a misleading reasoning ability, which is unlikely to generalize to other scenarios." In other words, the PGM corpus might make it too easy for learning systems to overfit to its particular, limited types of problems. </p>
                
            
                
                    <p tabindex="0">To remedy this limitation, Zhang et al. devised a new stochastic image-grammar method for generating a more diverse, comprehensive set of Raven's-like problems. They generated a new 70,000-problem corpus called RAVEN-smaller in size than PGM, but more diverse. Using a split with 42,000 training examples and 14,000 test examples, Zhang et al. compared some of the same deep learning methods examined by Barrett et al., plus their own novel "dynamic residual tree" method, which dynamically builds a tree-structured computation graph. They found that, whereas Barrett et al.'s WReN method had relatively high accuracy on the PGM dataset, its accuracy on the RAVEN dataset was 15%, barely above chance. The highest scoring method, with a 60% accuracy, was a residual network 73 (ResNet) using features computed by a dynamic residual tree. The authors also tested humans (college students) on the RAVEN corpus and found human accuracy to be about 84%. Other groups were able to improve on the machine-learning accuracy on RAVEN with larger, pretrained networks <a href="#BIBREF74"  id="BIBREF74-Deep Learning Approaches-ref">74</a> and contrastive learning, <a href="#BIBREF75"  id="BIBREF75-Deep Learning Approaches-ref">75</a> among other novel mechanisms. </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-5-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig5.webp" alt="Figure 5" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 5. Illustration of answer-generation method for RAVEN dataset. Given the correct answer (here, black pentagon at top), each incorrect candidate answer is generated by changing one attribute of the correct answer. Adapted from Ref. 76.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">However, Hu et al. <a href="#BIBREF76"  id="BIBREF76-Deep Learning Approaches-ref">76</a> discovered a major flaw in the RAVEN dataset. Recall that there are eight candidate answers, only one of which is correct. In RAVEN, the seven incorrect answers were gen- erated by randomly modifying a single attribute of the correct answer, as illustrated in <a href="#FIGREF5"  id="FIGREF5-Deep Learning Approaches-ref">Figure 5</a> . In the <a href="#FIGREF0"  id="FIGREF0-Deep Learning Approaches-ref">figure, answer #1</a> is the correct answer, and each of the other answers differs from it in exactly one attribute. But this allows a possible shortcut to determining the correct answer: just take the majority vote among the figures' attributes, here, shape, fill-pattern, and size. Indeed, when Hu et al. trained a ResNet only on the eight answer panels in each training example in RAVEN (i.e., leaving out the matrix itself), the ResNet achieved an accuracy of about 90%, competitive with the state of the art reported in the original RAVEN paper. This discovery casts doubt on previous claims that deep neural networks that perform well on the RAVEN dataset are actually solving the task that the authors intended them to solve. </p>
                
            
                
                    <p tabindex="0">Hu et al. proposed a more complex sampling method to create what they claimed was an "unbiased answer set" for each problem; the result is what they call "impartial-RAVEN." The authors showed that some deep learning methods reported to do well on RAVEN did much more poorly on impartial-RAVEN. Hu et al. also described a new method that outperformed other methods. Impartial-RAVEN (previously called "balanced-RAVEN") has become the subject of further competition for state-of-the-art accuracy. <a href="#BIBREF77"  id="BIBREF77-Deep Learning Approaches-ref">77,</a> <a href="#BIBREF78"  id="BIBREF78-Deep Learning Approaches-ref">78</a> In addition, Webb et al. <a href="#BIBREF79"  id="BIBREF79-Deep Learning Approaches-ref">79</a> demonstrated a mechanism for variable-binding in a neural-network system that performed well on simple abstraction problems including simplified versions of RPM. </p>
                
            
                
                    <p tabindex="0">In this section, I have reviewed some recent work on using deep learning for abstraction and analogy-making in Raven-like problems. While this is only one example of work on deep learning in this area, it serves as an illustrative microcosm, and highlights some of the advantages and limitations of such approaches. On the advantages side, deep learning avoids some of the issues I discussed with SME and Copycat. Deep learning approaches learn end-to-end from pixels, so avoid relying on built-in knowledge. Unlike SME, there is no separation of representation-building and mapping, and no semi-exhaustive search over possible matchings. However, these advantages are at the expense of two major limitations: the need for a huge training set, and the lack of transparency in what has been learned and how it is applied to new problems. The procedural generation of problems leaves open possibility of biases that allow shortcuts-the networks learn ways of performing well on the task that do not require the abstraction and analogy-making mechanisms that we assume humans bring to the task, and that we are trying to capture in machines. This can be seen in a simple way with biases in the RAVEN set, but such biases can be more subtle and hard to perceive, and have been identified time and again in machine-learning systems. <a href="#BIBREF80"  id="BIBREF80-Deep Learning Approaches-ref">[80]</a> <a href="#BIBREF81"  id="BIBREF81-Deep Learning Approaches-ref">[81]</a> <a href="#BIBREF82"  id="BIBREF82-Deep Learning Approaches-ref">[82]</a> It is not clear that benchmarks such as balanced-Raven are free from such biases. This, coupled with the lack of transparency about how the networks accomplish their tasks, makes it unclear what these networks have learned, and what features they are basing their decisions on. </p>
                
            
                
                    <p tabindex="0">All these lead to the conclusion that accuracy alone can be a misleading measure of performance. I will propose additional evaluation metrics that might be more informative in the Discussion section. </p>
                
            
                
                    <p tabindex="0">Even setting aside these problems with biases and lack of transparency, the approach of training such a system with a large set of examples is questionable, given the original intention of RPMs as a way of measuring general human intelligence. Humans do not need to train extensively in advance to perform well on RPMs; in fact, doing so would invalidate the test's ability to measure general intelligence. Rather, the idea is that humans learn how to do successful abstraction and analogy-making in the world, and intelligence tests such as Raven's are meant to be a measure of these general skills. In contrast, the neural networks that do well on Raven's test after extensive training are unable to transfer any of these skills to any other task. It is not clear that such approaches bring us any closer to the goal of giving machines general abilities for abstraction and analogy-making. </p>
                
            
                
                    <p tabindex="0">A potentially more promising (though less explored) set of approaches to abstraction uses metalearning <a href="#BIBREF83"  id="BIBREF83-Deep Learning Approaches-ref">83</a> -enabling a neural network to adapt to new tasks by training it on a training set of tasks rather than only on examples from a single task. Several metalearning approaches for few-shot learning have been evaluated on Lake et al.'s Omniglot dataset, <a href="#BIBREF84"  id="BIBREF84-Deep Learning Approaches-ref">84</a> among other benchmarks. <a href="#BIBREF85"  id="BIBREF85-Deep Learning Approaches-ref">[85]</a> <a href="#BIBREF86"  id="BIBREF86-Deep Learning Approaches-ref">[86]</a> <a href="#BIBREF87"  id="BIBREF87-Deep Learning Approaches-ref">[87]</a> <a href="#BIBREF88"  id="BIBREF88-Deep Learning Approaches-ref">[88]</a> Metalearning methods produced substantially improved generalization abilities for agents in a grounded-language task in a simple simulated environment, <a href="#BIBREF89"  id="BIBREF89-Deep Learning Approaches-ref">89</a> and improved few-shot learning accuracy in a simplified version of Bongard problems. <a href="#BIBREF90"  id="BIBREF90-Deep Learning Approaches-ref">90</a> However, at the time of this writing, metalearning approaches have not yet been explored for other abstraction and analogy domains. Another proposed approach is that of "metamapping" <a href="#BIBREF91"  id="BIBREF91-Deep Learning Approaches-ref">91</a> that directly maps a representation of one task to a representation of a related task. </p>
                
            
        
            <h2 id="section-body-6" tabindex="0">Probabilistic Program Induction </h2>
            
            
            
                
                    <p tabindex="0">So far I have covered methods illustrating symbolic and deep learning approaches. In this section I will describe a different family of methods for concept learning and analogy-making: probabilistic program-induction methods, in which a space of possible programs is defined (often via a program grammar) and a probability distribution is computed over this space with respect to a given task. Solving the task amounts to sampling from the space of possible programs guided by this probability distribution. Here, the task is concept induction, and a concept is identified with a program. I will illustrate probabilistic program induction by describing its application to two task domains: (1) recognizing and generating handwritten characters, and (2) solving Bongard problems. As I will discuss, program-induction methods can combine both symbolic and neural-network representations. </p>
                
            
                
                    <p tabindex="0">Recognizing and generating handwritten characters Lake et al. <a href="#BIBREF84"  id="BIBREF84-Probabilistic Program Induction-ref">84</a> applied probabilistic program induction to the task of learning visual concepts of handwritten characters from 50 different writing systems-the "Omniglot" task. The goal was to study one-shot learning-learning a concept from a single example. Here, learning a concept means seeing one example of a handwritten character and being able to not only recognize new examples of this character but also being able to generate new examples. In this work, a concept is represented as a program that generates examples of the concept. </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-6-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig6.webp" alt="Figure 6" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 6. Illustration of the generative model used by Lake et al. for generating Omniglot characters, given a set of primitive pen strokes. To generate a new character, first sample the number of parts; then sample the number of subparts that will make up each part; then sample these subparts from the primitives set; then sample possible sequences of the subparts to make a part; then sample relations between parts (from a library of possible relations). This sequence of samples is the program for generating the character concept. To generate a token of the character concept (i.e., a rendered character), run the character-concept program with motor noise added to selected points. Adapted from Ref. 84.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">In Omniglot, a program representing a concept consists of a hierarchical character-generation procedure. A character consists of a number of parts (complete pen strokes), each of which itself is made of subparts (movements separated by pauses of the pen). The generation procedure consists of sampling from a hierarchy of generative models describing the probabilities of the subparts, parts, and relationships among them. See <a href="#FIGREF6"  id="FIGREF6-Probabilistic Program Induction-ref">Figure 6</a> for an example. </p>
                
            
                
                    <p tabindex="0">How is this hierarchy of generative models learned? In Ref. <a href="#BIBREF84"  id="BIBREF84-Probabilistic Program Induction-ref">84</a> the authors describe a system that obtains prior knowledge from a training set of human-drawn characters spanning 30 alphabets (taken from the Omniglot dataset). The training set included the human-drawn pen strokes for each character, as well as the final character images. The learning system collected a library of primitive pen strokes and learned probability distributions over features of these pen strokes (e.g., starting positions). It also learned transition probabilities between strokes and probabilities over types of relations among the strokes, among other probabilities. All in all, this collection of probabilities is the prior knowledge of the system. </p>
                
            
                
                    <p tabindex="0">The system used this prior knowledge in a Bayesian framework for the tasks of one-shot classification and generation of characters not contained in the training set. (The authors also explored related tasks but I will not cover those here.) In the one-shot classification task, the system was presented with a single test image I (t ) of a new character of class t, along with 20 distinct characters I (c) in the same alphabet produced by human drawers. Only one of the 20 was the same class as I (t ) . The task was to choose that character from the 20 choices. </p>
                
            
                
                    <p tabindex="0">The Omniglot system computed an approximation to the probability P(I (t ) |I (c) ) for each of the 20 I (c) , and chose the I (c) that yielded the highest probability. By Bayes rule, P(I (t ) |I (c) ) ∝ P(I (c) |I (t ) )P(I (t ) ). </p>
                
            
                
                    <p tabindex="0">The term P(I (t ) ) is approximated by a probabilistic search method to generate a program to represent I (t ) of the form shown in <a href="#FIGREF6"  id="FIGREF6-Probabilistic Program Induction-ref">Figure 6</a> ; the prior probabilities learned from the original training set can be used to approximate P(I (t ) ). The term P(I (c) |I (t ) ) can be approximated by attempts to "refit" the program representing I (t ) to I (c) ; see Ref. <a href="#BIBREF92"  id="BIBREF92-Probabilistic Program Induction-ref">92</a> for details. </p>
                
            
                
                    <p tabindex="0">In the experiments reported by Lake et al., <a href="#BIBREF84"  id="BIBREF84-Probabilistic Program Induction-ref">84</a> the Omniglot system's performance on one-shot classification matched or exceeded that of the humans tested on this task. </p>
                
            
                
                    <p tabindex="0">The one-shot generation task was, given an example image I (c) of a hand-drawn character of class c, to generate a new image of a character of class c. The Omniglot system did this by first searching to find a program representing I (c) as above, and then running this program as in <a href="#FIGREF6"  id="FIGREF6-Probabilistic Program Induction-ref">Figure 6</a> to generate a new example. Lake et al. enlisted human judges to compare the characters generated by their system with those generated under the same one-shot conditions by humans-what they called a "visual Turing test." The judges were typically not able to distinguish between machine-generated and human-generated characters. </p>
                
            
                
                    <p tabindex="0">Feinman and Lake <a href="#BIBREF93"  id="BIBREF93-Probabilistic Program Induction-ref">93</a> proposed an interesting "neurosymbolic" extension of Lake et al.'s system that integrated neural networks with probabilistic programs to learn generative models of handwritten characters. </p>
                
            
        
            <h2 id="section-body-7" tabindex="0">Solving Bongard Problems </h2>
            
            
            
                
                    <p tabindex="0">The second example of recent work on probabilistic program induction is Depeweg et al.'s system for solving Bongard programs. <a href="#BIBREF94"  id="BIBREF94-Solving Bongard Problems-ref">94</a> Here, the idea is to induce a rule rather than a runnable program, but the general idea is the same. </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-7-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig7.webp" alt="Figure 7" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 7. Four sample Bongard problems. Adapted from Foundalis’ website.95</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">Bongard problems are visual concept recognition tasks, first presented in Bongard's 1970 book Pattern Recognition. <a href="#BIBREF96"  id="BIBREF96-Solving Bongard Problems-ref">96</a> <a href="#FIGREF7"  id="FIGREF7-Solving Bongard Problems-ref">Figure 7</a> shows four sample problems from Bongard's collection. For each problem the task is to identify the concept that distinguishes the set of six frames on the left from the set of six on the right (e.g., large vs. small or three vs. four). Often the concept is simple to express, but is represented quite abstractly in the figures (e.g., BP #91). </p>
                
            
                
                    <p tabindex="0">Bongard devised 100 such problems as a challenge for AI systems. Foundalis created a website <a href="#BIBREF95"  id="BIBREF95-Solving Bongard Problems-ref">95</a> to make available the original 100 problems plus many additional Bongard-like problems created by other people. It is notable that in the decades since Bongard's book was published, no artificial vision system has come close to solving all of Bongard's original problems. </p>
                
            
                
                    <p tabindex="0">Hofstadter has written extensively about Bongard problems, starting with his 1979 book Gödel, Escher, Bach: an Eternal Golden Braid, <a href="#BIBREF46"  id="BIBREF46-Solving Bongard Problems-ref">46</a> in which he sketched a rough architecture for solving them. (Notably, this sketch became the basis for the active symbol architecture that I described above.) </p>
                
            
                
                    <p tabindex="0">In contrast to the attention garnered in the AI community by RPM, Bongard problems have seen less attention. In a 1996 paper, Saito and Nakano <a href="#BIBREF97"  id="BIBREF97-Solving Bongard Problems-ref">97</a> showed that an inductive logic programming approach could solve 41 of the original 100 problems, starting not from the raw pixels but from logic formulas created by humans to represent each problem. In his 2006 PhD dissertation, Foundalis <a href="#BIBREF98"  id="BIBREF98-Solving Bongard Problems-ref">98</a> described the construction of a "cognitive architecture inspired by Bongard's problems," whose input was raw pixels of the 12 frames and whose output was an English phrase describing one or both sides of the problem. Foundalis' architecture was meant to model human concept induction in general rather than Bongard problems specifically, and was able to reliably solve about 10 problems. </p>
                
            
                
                    <p tabindex="0">Here, I will describe Depeweg et al.'s <a href="#BIBREF94"  id="BIBREF94-Solving Bongard Problems-ref">94</a> probabilistic rule-induction approach, which was inspired by the Omniglot system described above. Depeweg et al.'s system's task was to input the raw pixels of the 12 frames, and to output a rule R-in a logic-like language-that is true of all frames on one side (left or right) and none of the frames on the other side. (Note that this differs from Bongard's original task, which was to output English-language expressions contrasting the left and right sets of boxes.) </p>
                
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-7-2" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig8.webp" alt="Figure 8" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 8. (A) Human-designed grammar for rule induction. (B) Sample Bongard problem, with rule derivation and corresponding rule expression (the left side has objects that contain triangles). Adapted from Ref. 94.</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">In Depeweg et al.'s system, the space of possible rules is given by a human-designed grammar, given in <a href="#FIGREF8"  id="FIGREF8-Solving Bongard Problems-ref">Figure 8A</a> . A rule can be derived from this grammar by choosing one of the sides (right or left), starting with the start symbol S and probabilistically choosing one of the possible expansions of that symbol (e.g., S → ∃(L)), then probabilistically choosing possible expansions of the symbols in that expression (e.g., L → Contains(L)) and so on until all the variables have been expanded. </p>
                
            
                
                    <p tabindex="0">Given a Bongard problem, the goal is to search the possible space of rules to find the most probable rule. The authors define a probability distribution P(R|E, G) over possible rules. Here, R is a rule, E = (E LEFT , E RIGHT ) is the set of 12 input frames, and G is the grammar. By Bayes rule, this probability distribution can be factored into prior probability P(R|G) and likelihood P(E|R): </p>
                
            
        
            <h2 id="section-body-8" tabindex="0">P(R|E, G) ∝ P(E|R)P(R|G). </h2>
            
            
            
                
                    <p tabindex="0">The authors define the likelihood P(E|R) as equal to 1 if two conditions are met, and equal to 0 otherwise. The conditions are (1) R is true of all the frames on one side (left or right) and none of the frames on the other side, and (2) the frames in E are "informative" about R, meaning that the frames must contain the objects or relationships that are mentioned in the rule. For example, the rule "there exist triangles or squares inside circles" is true of all the frames on the left side of <a href="#FIGREF8"  id="FIGREF8-P(R|E, G) ∝ P(E|R)P(R|G).-ref">Figure 8B</a> , but since no squares actually appear, these frames are not considered to be informative about the rule, and the rule would be given zero likelihood. The prior probability term P(R|G) is defined by the authors in a way that favors shorter rules, along with some other structural properties of rules (see Ref. <a href="#BIBREF94"  id="BIBREF94-P(R|E, G) ∝ P(E|R)P(R|G).-ref">94</a>  </p>
                
            
        
            <h2 id="section-body-9" tabindex="0">For Details). </h2>
            
            
            
                
                    <p tabindex="0">Given the raw pixels of a Bongard problem, Depeweg et al.'s system applies simple image processing operations to extract objects, attributes, and relationships in each frame. The system was given a small repertoire of object, attribute, and relation types that it was able to extract. The authors note, "Our aim was not to build a general vision system, instead we decided to focus on these visual shapes, properties and relations that appear in many of the Bongard problems and hence seem to be likely candidates for a natural vocabulary from which relevant visual concepts can be built." The limitations of the system's repertoire means that their system is able to deal with only a subset of 39 of the 100 original problems. Once the image processing has been completed on the input Bongard problem, the system searches for a rule by sampling (using a form of the Metropolis-Hastings algorithm) from possible rules under the probability distribution described above. The system is allowed 300,000 samples to find a compatible rule for each Bongard problem. Of the 39 problems the grammar could deal with, the system was able to find compatible rules for 35 of them. </p>
                
            
        
            <h2 id="section-body-10" tabindex="0">Summary Of Program-Induction Approaches </h2>
            
            
            
                
                    <p tabindex="0">The Omniglot-and Bongard-problem examples illustrate the promise and challenges of probabilistic program-induction approaches. Framing concept learning as the task of generating a program enables many of the advantages of programming in general, including flexible abstraction, reusability, modularity, and interpretability. Bayesian inference methods seamlessly combine prior knowledge and preferences with likelihoods, and enable powerful sampling methods. Such advantages have fueled strong interest and progress in probabilistic program induction in recent years, including combining program induction with neural networks, reinforcement learning, and other methods, <a href="#BIBREF99"  id="BIBREF99-Summary Of Program-Induction Approaches-ref">[99]</a> <a href="#BIBREF100"  id="BIBREF100-Summary Of Program-Induction Approaches-ref">[100]</a> <a href="#BIBREF101"  id="BIBREF101-Summary Of Program-Induction Approaches-ref">[101]</a> <a href="#BIBREF102"  id="BIBREF102-Summary Of Program-Induction Approaches-ref">[102]</a> as well as incorporating program induction with methods inspired by neuroscience <a href="#BIBREF103"  id="BIBREF103-Summary Of Program-Induction Approaches-ref">103</a> and psychology. <a href="#BIBREF104"  id="BIBREF104-Summary Of Program-Induction Approaches-ref">104,</a> <a href="#BIBREF105"  id="BIBREF105-Summary Of Program-Induction Approaches-ref">105</a> The work of Lázaro-Gredilla et al. <a href="#BIBREF103"  id="BIBREF103-Summary Of Program-Induction Approaches-ref">103</a> is notable in that it combines probabilistic program induction with a neurally inspired model of visual perception and action in a physical robot. The work of Evans et al. <a href="#BIBREF104"  id="BIBREF104-Summary Of Program-Induction Approaches-ref">104</a> notably focuses on unsupervised program induction constrained by domain-independent properties of cognition. </p>
                
            
                
                    <p tabindex="0">Many challenges remain to make probabilistic program induction a more general-purpose AI method for concept learning, abstraction, and analogy. These methods currently need substantial built-in knowledge, structured by humans, in the form of the program primitives and grammar (the "domain-specific language") for a given problem. Moreover, these methods require humans to define prior probability and likelihood distributions over possible programs, which is not always a straightforward task. Perhaps most important, solving a given task can require an enormous amount of search in the space of possible programs, which currently limits scaling up such program-induction methods to more complex problems (though some of the hybrid methods cited above are focused on dealing with this combinatorial explosion of possibilities.) Finally, it remains to be seen whether the concept-as-program notion allows for the flexibility, extensibility, and analogy-making abilities that are the hallmark of human concepts. </p>
                
            
        
            <h2 id="section-body-11" tabindex="0">Abstraction And Reasoning Corpus </h2>
            
            
            
                
                    
                    
                    <!--Do not display image (display content instead)-->
                    
                        <!--placeholder image-->
                        
                            <!--figure with alt-text-->
                            
                                <figure id="fig-11-1" class="paper__figure" tabindex="0">
                                    <img src="/projects/papyrus/pap/papers/img/paper3/fig9.webp" alt="Figure 9" style="width:100%";>
                                    <figcaption class="paper__figure-caption">Figure 9. A sample ARC task. Adapted from Chollet.107 (Best viewed in color.)</figcaption>
                                </figure>
                            
                        
                    
                
            
                
                    <p tabindex="0">This section does not describe a new AI method for abstraction and analogy, but rather a new promising benchmark-the abstraction and reasoning corpus (ARC)-for evaluating these abilities. ARC was developed by Chollet as part of a project on how to measure intelligence in AI systems. <a href="#BIBREF106"  id="BIBREF106-Abstraction And Reasoning Corpus-ref">106</a> ARC is a collection of visual analogy "tasks." <a href="#FIGREF9"  id="FIGREF9-Abstraction And Reasoning Corpus-ref">Figure 9</a> gives a sample task from the corpus. The left side presents three "task demonstrations"; each of these consists of two grids with colored boxes. In each demonstration, you can think of the first (left) grid as "transforming" into the second (right) grid. The right side of <a href="#FIGREF9"  id="FIGREF9-Abstraction And Reasoning Corpus-ref">Figure 9</a> presents a "test"; the task is to transform this grid analogously to the demonstrations. This task could be thought of as one of "few-shot" learning-the demonstrations on the left are the training examples, and the grid on the right is a test example. </p>
                
            
                
                    <p tabindex="0">The ARC domain features visual analogies between idealized "situations" that can express abstract concepts in unlimited variations. In this way it exhibits some of the combined advantages of the other idealized domains I have discussed in this paper, such as Copycat's letter-string analogies, Bongard problems, and RPM. </p>
                
            
                
                    <p tabindex="0">Chollet manually designed 1000 ARC tasks, with the motivation of enabling a fair comparison of "general intelligence" between AI systems and humans-a comparison that does not involve language or other acquired human knowledge. Instead, ARC tasks are meant to rely only on the innate core knowledge systems proposed by Spelke, 9 which include intuitive knowledge about objects, agents and their goals, numerosity, and basic spatial-temporal concepts. </p>
                
            
                
                    <p tabindex="0">An important aspect of ARC is its relatively small size. Chollet made 400 tasks publicly available and reserved 600 tasks as a "hidden" evaluation set. In 2020, the Kaggle website hosted a 3-month "Abstraction and Reasoning Challenge" in which researchers were encouraged to submit programs to be evaluated on the hidden evaluation set. The best performing submissions had about 20% accuracy on a top three metric (three answers were allowed per task, and if one or more was correct, the task was considered to be solved). However, none of the submissions used an approach that was likely to be generalizable (Chollet, personal communication) . Thus the ARC challenge remains wide open. </p>
                
            
        
            <h2 id="section-body-12" tabindex="0">Discussion: How To Make Progress In Ai On Abstraction And Analogy </h2>
            
            
            
                
                    <p tabindex="0">In the sections above, I have described several diverse AI approaches to abstraction and analogy, including symbolic methods, deep learning, and probabilistic program induction. These approaches each have their own advantages and limitations. </p>
                
            
                
                    <p tabindex="0">Symbolic systems such as SME can be explicitly programmed with prior knowledge (in symbolic form) as well as with important heuristics of abstraction and analogy-making such as Gentner's systematicity principle. Symbolic representation methods such as predicate-logic or semantic networks offer unambiguous variables and variablebinding, clear type-token distinctions, and explicit measures of conceptual similarity and conceptual abstraction, among other facilities associated with human reasoning. These systems also have the advantage of interpretability, since their "reasoning" on a given problem is readable in symbolic form. However, representations that focus on the syntax of logic-like representations can suffer from brittleness; moreover symbolic approaches often require humans to create and structure substantial prior knowledge, and these systems often rely on semiexhaustive search. There have been several interesting neurosymbolic approaches that implement symbolic-like behavior in neural networks (e.g., see Refs. <a href="#BIBREF108"  id="BIBREF108-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">[108]</a> <a href="#BIBREF109"  id="BIBREF109-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">[109]</a> <a href="#BIBREF110"  id="BIBREF110-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">[110]</a> , but these remain limited in their generality and can suffer from some of the limitations of symbolic systems discussed above. </p>
                
            
                
                    <p tabindex="0">Active symbol architectures, such as the Copycat program, were claimed to address some of the limitations of purely symbolic methods <a href="#BIBREF39"  id="BIBREF39-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">39,</a> <a href="#BIBREF47"  id="BIBREF47-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">47</a> by enabling the system to actively build its representation of a situation in a workspace, via a continual interaction between bottom-up and top-down information processing, avoiding any kind of exhaustive search. However, Copycat and other examples of active symbol architectures remain dependent on prior knowledge provided and structured by humans, and these systems as yet have no mechanism for learning new permanent concepts. </p>
                
            
                
                    <p tabindex="0">Deep learning systems do not require the preprogrammed structured knowledge of symbolic systems; they are able to learn from essentially raw data. However, they require large training corpora, as well as significant retraining (and often retuning of hyperparameters) for each new task. In addition, they are susceptible to learning statistical shortcuts rather than the actual concepts that humans intend, and their lack of intepretability often makes it difficult to ascertain the extent to which they are actually performing abstraction or analogy. Moreover, if the goal is to imbue machines with general humanlike abstraction abilities, it does not make sense to have to train them on tens of thousands of examples, since the essence of abstraction and analogy is fewshot learning. While some metalearning systems have shown interesting performance on certain few-shot learning tasks, their overall generality and robustness still needs to be demonstrated. </p>
                
            
                
                    <p tabindex="0">Probabilistic program induction, by framing concept learning as the task of generating a program, enables many of the advantages of programming in general, including flexible abstraction, reusability, modularity, and interpretability. Moreover, Bayesian inference methods enable the combination of prior knowledge and preferences with likelihoods, and a probabilistic approach enables powerful sampling methods. However, like symbolic approaches, current program-induction approaches require significant human-engineered prior knowledge in the form of a domain-specific language. And the more expressive the language, the more daunting the combinatorial search problem these methods face. </p>
                
            
                
                    <p tabindex="0">Stepping back from any individual approach, it is difficult to assess how much general progress has been made on AI methods for abstraction and analogy, since each AI system is developed for and evaluated on a particular domain. Moreover, as I described in the sections above, these evaluations have largely relied on the system's accuracy on a particular set of test problems. What is missing are assessments based on generality across diverse domains, as well as the robustness of a given system to factors such as noise, variations on an abstract concept, or scaling of task complexity. In order to make further progress, we need to rethink how we choose or design domains and what evaluation criteria and evaluation processes we use. </p>
                
            
                
                    <p tabindex="0">The following are my own recommendations for research and evaluation methods to make quantifiable and generalizable progress in developing AI systems for abstraction and analogy. </p>
                
            
                
                    <p tabindex="0">Focus on idealized domains. AI researchers have used diverse idealized domains for developing and evaluating systems to perform abstraction and analogy, including RPM, letter-string analogy problems, the Omniglot challenge, and Bongard problems, among others (e.g., more recently, new idealized visual domains have been proposed in Refs. 90 and 103) . While some approaches have used natural image-or language-based domains, <a href="#BIBREF26"  id="BIBREF26-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">26,</a> <a href="#BIBREF27"  id="BIBREF27-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">27,</a> <a href="#BIBREF32"  id="BIBREF32-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">32,</a> <a href="#BIBREF60"  id="BIBREF60-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">60,</a> <a href="#BIBREF61"  id="BIBREF61-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">61,</a> <a href="#BIBREF111"  id="BIBREF111-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">111</a> there are advantages to using idealized nonlinguistic domains. In idealized domains, it is possible to be explicit about what prior knowledge is assumed, as opposed to the open-ended knowledge requirements of human language and imagery. Real-world image-or language-based tasks have much richer meanings to humans than to the machines processing this data; by avoiding such tasks we can avoid anthropomorphizing and overestimating what an AI system has actually achieved. There are risks with idealized domains, since it is not always clear that these domains capture the kind of real-world phenomena we want to model, but as in other sciences, the first step to progress is to isolate the phenomenon we are studying, in as idealized a form as possible. While I believe that important general cognitive abilities can be developed using these idealized challenges, others have argued that such abilities can be enabled only by exposing systems to much richer data or experience. <a href="#BIBREF112"  id="BIBREF112-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">112</a> Focus on "core knowledge." Chollet has suggested that challenge domains for assessing AI's "intelligence" should rely only on human "core knowledge" rather than acquired knowledge, such as language. <a href="#BIBREF106"  id="BIBREF106-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">106</a> Spelke and colleagues <a href="#BIBREF8"  id="BIBREF8-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">9</a> have proposed that human cognition is founded on of a set of four core knowledge systems, which include objects and intuitive physics; agents and goaldirectedness; numbers and elementary arithmetic; and spatial geometry of the environment (which includes relational concepts such as "in front of" or "contains). The idealized domains I have discussed above-especially the ARC domain-mostly rely only on such core knowledge. (Note that in the letter-string analogy domain, letters of the alphabet are used as idealized representatives of objects that can be related to other objects in specific ways; knowledge of language, such as how letters are used in words, or the visual characteristics of letters, are outside of the idealized domain. <a href="#BIBREF41"  id="BIBREF41-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">41 )</a> Restricting a domain's required prior knowledge to such concepts makes it possible to fairly compare performance among AI systems as well as between AI systems and humans. Indeed, I would argue that unless we create AI systems that can master such core, nonlinguistic knowledge, we have little hope of creating anything like human-level AI. </p>
                
            
                
                    <p tabindex="0">Evaluate systems across multiple domains. All of the domains I have discussed here capture interesting facets of abstraction and analogy-making, including the recognition of abstract similarity, conceptual extrapolation, and adaptation of knowledge to novel situations. However, the common practice of focusing on a single domain limits progress. AI research focusing exclusively on any one domain has the risk of "overfitting" to that domain. I believe that the research community needs to adopt a diverse suite of challenge domains on which systems can be evaluated for generality; such a strategy has a better chance to develop truly general and robust approaches. This is the strategy taken by the natural language processing community, for example, with the GLUE and SuperGLUE benchmarks, <a href="#BIBREF113"  id="BIBREF113-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">113,</a> <a href="#BIBREF114"  id="BIBREF114-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">114</a> though these benchmarks focus on tasks associated with large training sets and fixed test sets, which have allowed for successful solutions based on "shortcuts." <a href="#BIBREF81"  id="BIBREF81-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">81,</a> <a href="#BIBREF115"  id="BIBREF115-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">115,</a> <a href="#BIBREF116"  id="BIBREF116-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">116</a> For that reason I endorse a focus on tasks that do not allow large amounts of specific training data, as I detail next. </p>
                
            
                
                    <p tabindex="0">Focus on tasks that require little or no training. Since abstraction and analogy are at core concerned with flexibly mapping one's knowledge to new situations, it makes little sense to have to extensively train a system to deal with each new abstraction or analogy problem. Thus, I believe the research community should focus on tasks that do not require extensive training on examples from the domain itself. This echoes Chollet's criteria for such tasks: "It should not be possible to 'buy' performance on the benchmark by sampling unlimited training data. The benchmark should avoid tasks for which new data can be generated at will. It should be, in effect, a game for which it is not possible to practice in advance of the evaluation session." <a href="#BIBREF106"  id="BIBREF106-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">106</a> When humans make abstractions or analogies, it can be argued that the "training" for such abilities is the process of developing core concepts, much of which takes place in early childhood. <a href="#BIBREF7"  id="BIBREF7-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">8,</a> <a href="#BIBREF8"  id="BIBREF8-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">9,</a> <a href="#BIBREF117"  id="BIBREF117-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">117</a> Similarly, AI systems that can solve problems in idealized domains should require "training" only on the core concepts required in each domain. Rather than training and testing on the same task, enabling machines to have general abstraction abilities will require that they learn core concepts and then adapt that knowledge to a multitude of different tasks, without being trained specifically for any one of them. </p>
                
            
                
                    <p tabindex="0">Include generative tasks. Some idealized domains offer discriminative tasks-for example, RPM, in which the solver chooses from a set of candidate answers. Others, such as the letter-string analogies or ARC, are generative: the solver has to generate their own answer. Generative tasks are likely more resistant to shortcut learning than discriminative tasks, and systems that generate answers are in many cases more interpretable. Most importantly, if the space of problems is diverse enough, having to generate answers forces a deeper understanding of the task. </p>
                
            
                
                    <p tabindex="0">Evaluate systems on "hidden," human-curated, changing sets of problems. It is important that problems to be used for evaluation are not be made available to the developers of AI systems that will be evaluated on these problems. Furthermore, these evaluation sets should not remain fixed for a long period of time, since systems can indirectly "overfit" to a fixed set of evaluation problems, even when they are hidden. In addition, the evaluation problems should be created and curated by humans, rather than relying on automatic generation by algorithms; I described above how procedurally generated problems can unintentionally allow shortcut solutions; they can also allow a system to reverse-engineer the generating algorithm instead of being able to solve the problems in a general way. <a href="#BIBREF106"  id="BIBREF106-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">106</a> Of course, human-generated problems can also allow for shortcut solutions; thus evaluation sets need to be carefully curated to avoid shortcut solutions as much as possible, for example via adversarial filtering methods. <a href="#BIBREF118"  id="BIBREF118-Discussion: How To Make Progress In Ai On Abstraction And Analogy-ref">118</a> Evaluate systems on their robustness, not simply their accuracy. Like other research in AI, methods for abstraction and analogy are often evaluated on their accuracy on a set of test problems from a given domain. However, as I discussed above, measuring accuracy on a fixed set of test problems does not reveal possible shortcuts-strategies a system takes to solve problems that do not reflect the actual general abilities that the evaluation is meant to test. In order to make progress on general abstraction and analogy abilities, in addition to evaluating systems across multiple domains, we need to evaluate them along multiple dimensions of robustness. For example, the evaluation benchmarks should feature various kinds of challenges in order to measure a system's robustness to "noise" and other irrelevant distractions, and to variations in a given concept (e.g., if one example tests recognition of the abstract concept "monotonically increasing," other examples should test variations of this concept with different degrees of abstraction). Finally, the evaluation problems should also test a system's ability to scale to more complex examples of a given concept (e.g., if a system is able to recognize "monotonically increasing" with small number of elements, it should also be tested on the same concept with a larger number of elements, or with more complex elements). </p>
                
            
        
            <h2 id="section-body-13" tabindex="0">Conclusion </h2>
            
            
            
                
                    <p tabindex="0">In this paper I have argued that humanlike abstraction and analogy-making abilities will be key to constructing more general and trustworthy AI systems, ones that can learn from a small number of examples, robustly generalize, and reliably adapt their knowledge to diverse domains and modalities. I have reviewed several approaches to building systems with these abilities, including symbolic and "active symbol" approaches, deep learning, and probabilistic program induction. I have discussed advantages and current limitations of each of these approaches, and argued that it remains difficult to assess progress in this area due to the lack of evaluation methods addressing generality, robustness, and scaling abilities. Finally, I proposed several steps towards making quantifiable and generalizable progress, by designing appropriate challenge suites and evaluation methods. </p>
                
            
                
                    <p tabindex="0">The quest for machines that can make abstractions and analogies is as old as the AI field itself, but the problem remains almost completely open. I hope that this paper will help spur renewed interest and attention in the AI community to understanding these core abilities which form the foundations of general intelligence. </p>
                
            
        
            <h2 id="section-body-14" tabindex="0">SECTION </h2>
            
            
            
                
                    <p tabindex="0">doi: 10.1111/nyas.14619 </p>
                
            
                
                    <p tabindex="0">Ann. N.Y. Acad. Sci. 1505 (2021) 79-101 © 2021 New York Academy of Sciences. </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">Ann. N.Y. Acad. Sci. 1505 (2021) 79-101 © 2021 New York Academy of Sciences.17496632, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
                
                    <p tabindex="0">, 2021, 1, Downloaded from https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14619 by <shibboleth>-member@chalmers.se, Wiley Online Library on [03/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License </p>
                
            
        

        <footer>
            
                <h2 id="paper-references" tabindex="0">References</h2>
                <ul tabindex="0">
                    
                        <li id="BIBREF0">Hofstadter, D. & E. Sander. 2013. Surfaces and Essences. New York: Basic Books.<br/><i><a href="#BIBREF0-Introduction-ref">Introduction</a>, <a href="#BIBREF0-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF1">McCarthy, J., M.L. Minsky, N. Rochester & C.E. Shan- non. 2006. A proposal for the Dartmouth summer research project in artificial intelligence, 1955. AI Mag. 27: 12-14.</li>
                    
                        <li id="BIBREF10">Rosch, E. 1999. "Principles of categorization." In Concepts: Core Readings. E. Margolis & S. Laurence, Eds.: 189-206. Cambridge, MA: MIT Press.<br/><i><a href="#BIBREF10-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF100">Ellis, K., A. Solar-Lezama, D. Ritchie & J.B. Tenenbaum. 2018. Learning to infer graphics programs from hand- drawn images. In Advances in Neural Information Process- ing Systems, Montreal, Canada.<br/><i><a href="#BIBREF100-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF101">Ganin, Y., T. Kulkarni, I. Babuschkin, et al. 2018. Synthesiz- ing programs for images using reinforced adversarial learn- ing. In Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholm, Sweden, Vol. 4.<br/><i><a href="#BIBREF101-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF102">Ellis, K., C. Wong, M. Nye, et al. 2020. DreamCoder: grow- ing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. Preprint, arXiv:2006.08381.<br/><i><a href="#BIBREF102-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF103">Lázaro-Gredilla, M., D. Lin, J.S. Guntupalli & D. George. 2019. Beyond imitation: zero-shot task transfer on robots by learning concepts as cognitive programs. Sci. Rob. 4. doi:10.1126/scirobotics.aav3150.<br/><i><a href="#BIBREF103-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF104">Evans, R., J. Hernández-Orallo, J. Welbl, et al. 2021. Making sense of sensory input. Artif. Intell. 293: 103438.<br/><i><a href="#BIBREF104-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF105">Rule, J.S., J.B. Tenenbaum & S.T. Piantadosi. 2020. The child as hacker. Trends. Cogn. Sci. 24: 900-915.<br/><i><a href="#BIBREF105-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                        <li id="BIBREF106">Chollet, F. 2019. On the measure of intelligence. Preprint, arXiv:1911.01547.<br/><i><a href="#BIBREF106-Abstraction%20And%20Reasoning%20Corpus-ref">Abstraction And Reasoning Corpus</a>, <a href="#BIBREF106-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF107">Chollet, F. 2019. The abstraction and reasoning corpus (ARC). <a href="https://github.com/fchollet/ARC." target="_blank">https://github.com/fchollet/ARC.</a></li>
                    
                        <li id="BIBREF108">Doumas, L.A.A., G. Puebla, A.E. Martin & J.E. Hummel. 2020. Relation learning in a neurocomputational archi- tecture supports cross-domain transfer. In Proceedings of the 42nd Annual Meeting of the Cognitive Science Society, Toronto, Canada, 932-937.<br/><i><a href="#BIBREF108-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF109">Mao, J., C. Gan, P. Kohli, et al. 2019. The neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA.<br/><i><a href="#BIBREF109-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF11">Gopnik, A. 2003. "The theory theory as an alternative to the innateness hypothesis." In Chomsky and His Critics. L.M. Antony & N. Hornstein, Eds.: 238-254. Hoboken: Wiley- Blackwell.<br/><i><a href="#BIBREF11-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF110">Smolensky, P. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artif. Intell. 46: 159-216.<br/><i><a href="#BIBREF110-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF111">Andonian, A., C. Fosco, M. Monfort, et al. 2020. We have so much in common: Modeling semantic relational set abstractions in videos. Preprint, arXiv:2008.05596.<br/><i><a href="#BIBREF111-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF112">Hill, F., A. Lampinen, R. Schneider, et al. 2020. Envi- ronmental drivers of systematicity and generalization in a situated agent. In Proceedings of the International Con- ference on Learning Representations, ICLR, Addis Ababa, Ethiopia.<br/><i><a href="#BIBREF112-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF113">Wang, A., A. Singh, F. Michael, et al. 2018. GLUE: a multi- task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Work- shop on Blackbox NLP: Analyzing and Interpreting Neural Networks for NLP, 353-355. Stroudsburg, PA: Association for Computational Linguistics.<br/><i><a href="#BIBREF113-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF114">Wang, A., Y. Pruksachatkun, N. Nangia, et al. 2019. Super- GLUE: a stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32, Vancouver, BC, Canada, 3266-3280.<br/><i><a href="#BIBREF114-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF115">Linzen, T. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5210-5217. Stroudsburg, PA: Association for Computational Linguistics.<br/><i><a href="#BIBREF115-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF116">McCoy, T., E. Pavlick & T. Linzen. 2020. Right for the wrong reasons: diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, PA: Association for Computational Linguistics.<br/><i><a href="#BIBREF116-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF117">Mandler, J.M. 1992. How to build a baby: II. Conceptual primitives. Psychol. Rev. 99: 587-604.<br/><i><a href="#BIBREF117-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF118">Sakaguchi, K., R. Le Bras, C. Bhagavatula & Y. Choi. 2020. Winogrande: an adversarial Winograd schema challenge at scale. In Proceedings of the National Conference on Artificial Intelligence, AAAI, New York, 8732-8740.<br/><i><a href="#BIBREF118-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF12">Barsalou, L.W. 2009. Simulation, situated conceptualiza- tion, and prediction. Philos. Trans. R. Soc. B 364: 1281- 1289.<br/><i><a href="#BIBREF12-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF13">Lakoff, G. 1993. "The contemporary theory of metaphor." In Metaphor and Thought. A. Ortony, Ed.: 202-251. Cam- bridge: Cambridge University Press.<br/><i><a href="#BIBREF13-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF14">Goodman, N., J.B. Tenenbaum & T. Gerstenberg. 2015. "Concepts in a probabilistic language of thought." In The Conceptual Mind: New Directions in the Study of Concepts. E. Margolis & S. Laurence, Eds.: 623-654. Cambridge, MA: MIT Press.<br/><i><a href="#BIBREF14-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF15">Fauconnier, G. 1997. Mappings in Thought and Language. Cambridge: Cambridge University Press.<br/><i><a href="#BIBREF15-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF16">Johnson, M. 2017. Embodied Mind, Meaning, and Reason: How Our Bodies Give Rise to Understanding. Chicago, IL: University of Chicago Press.<br/><i><a href="#BIBREF16-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF17">Lake, B.M., T.D. Ullman, J.B. Tenenbaum & S.J. Gershman. 2017. Building machines that learn and think like people. Behav. Brain. Sci. 40: E253.<br/><i><a href="#BIBREF17-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF18">Lakoff, G. 2012. "The neural theory of metaphor." In The Cambridge Handbook of Metaphor and Thought. R.W.<br/><i><a href="#BIBREF18-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF2">Hofstadter, D.R. 2001. "Analogy as the core of cognition." In The Analogical Mind: Perspectives from Cognitive Sci- ence. D. Gentner, K.J. Holyoak & B.N. Kokinov, Eds.: 499- 538. Cambridge, MA: MIT Press.<br/><i><a href="#BIBREF2-Introduction-ref">Introduction</a>, <a href="#BIBREF2-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF20">Gentner, D. & C. Hoyos. 2017. Analogy and abstraction. Top. Cogn. Sci. 9: 672-693.<br/><i><a href="#BIBREF20-Introduction-ref">Introduction</a>, <a href="#BIBREF20-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF21">Holyoak, K.J. 1984. "Analogical thinking and human intel- ligence." In Advances in the Psychology of Human Intelli- gence. Vol. 2. R.J. Sternberg, Ed.: 199-230. Hillsdale, NJ: Erllbaum.<br/><i><a href="#BIBREF21-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF22">Schweber, S.S. 1980. Darwin and the political economists: divergence of character. J. Hist. Biol. 13: 195-289.<br/><i><a href="#BIBREF22-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF23">von Neumann, J. 1958. The Computer and the Brain. New Haven, CT: Yale University Press.<br/><i><a href="#BIBREF23-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF24">Turner, M. & G. Fauconnier. 1997. "Conceptual integration in counterfactuals." In Discourse and Cognition. Thousand Oaks, CA: Sage Publications.<br/><i><a href="#BIBREF24-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF25">Bartha, P. 2019. "Analogy and analogical reasoning." In The Stanford Encyclopedia of Philosophy. Spring 2019 ed. E.N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/spr2019/entries/reasoning-analogy." target="_blank">https://plato.stanford.edu/archives/spr2019/entries/reasoning-analogy.</a><br/><i><a href="#BIBREF25-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a></i></li>
                    
                        <li id="BIBREF26">Mikolov, T., W.T. Yih & G. Zweig. 2013. Linguistic regular- ities in continuous space word representations. In Proceed- ings of the North American Chapter of the Association for Computational Linguistics Proceedings of the North Ameri- can Chapter of the Association for Computational Linguis- tics, 746-751. Stroudsburg, PA: Association for Computa- tional Linguistics.<br/><i><a href="#BIBREF26-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a>, <a href="#BIBREF26-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF26-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF27">Sadeghi, F., C.L. Zitnick & A. Farhadi. 2015. VISAL- OGY: answering visual analogy questions. In Advances in Neural Information Processing Systems, Montreal, Quebec, Canada, 1882-1890.<br/><i><a href="#BIBREF27-Abstraction%20And%20Analogy-Making%20In%20Intelligence-ref">Abstraction And Analogy-Making In Intelligence</a>, <a href="#BIBREF27-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF27-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF28">Evans, T.G. 1968. "A program for the solution of a class of geometric-analogy intelligence-test questions." In Seman- tic Information Processing. M. Minsky, Ed.: Cambridge, MA: MIT Press.<br/><i><a href="#BIBREF28-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF29">Winston, P.H. 1980. Learning and reasoning by analogy. Commun. ACM. 23: 689-703.<br/><i><a href="#BIBREF29-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF3">Mitchell, M. 2019. Artificial Intelligence: A Guide for Think- ing Humans. New York: Farrar, Straus, and Giroux.<br/><i><a href="#BIBREF3-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF30">Falkenhainer, B., K.D. Forbus & D. Gentner. 1986. The structure-mapping engine. In Proceedings of the National Conference on Artificial Intelligence, AAAI, Philadelphia, PA, 272-277.<br/><i><a href="#BIBREF30-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF31">Gentner, D. 1983. Structure-mapping: a theoretical frame- work for analogy. Cogn. Sci. 7: 155-170.<br/><i><a href="#BIBREF31-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF32">Blass, J.A. & K.D. Forbus. 2016. Modeling commonsense reasoning via analogical chaining: a preliminary report. In Proceedings of the 38th Annual Meeting of the Cognitive Sci- ence Society, Philadelphia, PA.<br/><i><a href="#BIBREF32-Symbolic%20Methods-ref">Symbolic Methods</a>, <a href="#BIBREF32-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF33">Dehghani, M., K.D. Forbus, E. Tomai & M. Klenk. 2008. An integrated reasoning approach to moral decision mak- ing. In Proceedings of the National Conference on Artificial Intelligence, AAAI, Chicago, IL, 1280-1286.<br/><i><a href="#BIBREF33-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF34">Forbus, K.D., J. Usher & E. Tomai. 2005. Analogical learn- ing of visual/conceptual relationships in sketches. In Pro- ceedings of the National Conference on Artificial Intelligence, AAAI, Pittsburgh, PA, 202.<br/><i><a href="#BIBREF34-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF35">Lovett, A. & K.D. Forbus. 2017. Modeling visual problem solving as analogical reasoning. Psychol. Rev. 124: 60-90.<br/><i><a href="#BIBREF35-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF36">Forbus, K.D., A. Lovett, K. Lockwood, et al. 2008. CogS- ketch. In Proceedings of the National Conference on Artifi- cial Intelligence, AAAI, Chicago, IL, 1878-1879.<br/><i><a href="#BIBREF36-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF37">Perkins, T. 2020. Joe Biden positions himself as "bridge" to next generation at Michigan rally. The Guardian. Last accessed June 10, 2021. <a href="https://www.theguardian.com/us-news/2020/mar/10/joe-biden-michigan-rally-" target="_blank">https://www.theguardian.com/us-news/2020/mar/10/joe-biden-michigan-rally-</a> kamala-harris-cory-booker.<br/><i><a href="#BIBREF37-Symbolic%20Methods-ref">Symbolic Methods</a></i></li>
                    
                        <li id="BIBREF38">Forbus, K.D., D. Gentner, A.B. Markman & R.W. Ferguson. 1998. Analogy just looks like high level perception: why a domain-general approach to analogical mapping is right. J. Exp. Theor. Artifi. Intell. 10: 231-257.<br/><i><a href="#BIBREF38-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF39">Chalmers, D.J., R.M. French & D.R. Hofstadter. 1992. High- level perception, representation, and analogy: a critique of artificial intelligence methodology. J. Exp. Theor. Artifi. Intell. 4: 185-211.<br/><i><a href="#BIBREF39-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a>, <a href="#BIBREF39-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF4">Alcorn, M.A., Q. Li, Z. Gong, et al. 2019. Strike (with) a pose: neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, Xi'an, China, 4840-4849.<br/><i><a href="#BIBREF4-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF40">French, R.M. 2002. The computational modeling of analogy-making. Trends. Cogn. Sci. 6: 200-205.<br/><i><a href="#BIBREF40-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF41">Hofstadter, D.R. & M. Mitchell. 1994. "The Copycat project: a model of mental fluidity and analogy-making." In Advances in Connectionist and Neural Computation The- ory. Vol. 2. K.J. Holyoak & J.A. Barnden, Eds.: 31-112. Nor- wood, NJ: Ablex Publishing Corporation.<br/><i><a href="#BIBREF41-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a>, <a href="#BIBREF41-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF42">Holyoak, K.J. & P. Thagard. 1989. Analogical map- ping by constraint satisfaction. Cogn. Sci. 13: 295- 355.<br/><i><a href="#BIBREF42-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF43">Hummel, J.E. & K.J. Holyoak. 1996. LISA: a computational model of analogical inference and schema induction. In Proceedings of the Eighteenth Annual Meeting of the Cogni- tive Science Society, 352-357. San Diego, CA: University of California.<br/><i><a href="#BIBREF43-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF44">Crouse, M., C. Nakos, I. Abdelaziz & K.D. Forbus. 2020. Neural analogical matching. Preprint, arXiv:2004. 03573.<br/><i><a href="#BIBREF44-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a>, <a href="#BIBREF44-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF45">Kokinov, B.N. & A. Petrov. 2001. "Integrating memory and reasoning in analogy-making: the AMBR model." In The Analogical Mind: Perspectives from Cognitive Science. Cam- bridge, MA: MIT Press.<br/><i><a href="#BIBREF45-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF46">Hofstadter, D.R. 1979. Gödel, Escher, Bach: an Eternal Golden Braid. New York: Basic Books.<br/><i><a href="#BIBREF46-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a>, <a href="#BIBREF46-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a></i></li>
                    
                        <li id="BIBREF47">Hofstadter, D.R. 1995. Fluid Concepts and Creative Analo- gies: Computer Models of the Fundamental Mechanisms of Thought. New York: Basic Books.<br/><i><a href="#BIBREF47-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a>, <a href="#BIBREF47-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF48">Mitchell, M. 1993. Analogy-Making as Perception: A Com- puter Model. Cambridge, MA: MIT Press.<br/><i><a href="#BIBREF48-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF49">Hofstadter, D.R. 1985. Analogies and roles in human and machine thinking. Metamagical Themas. New York: Basic Books.<br/><i><a href="#BIBREF49-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF5">Szegedy, C., W. Zaremba, I. Sutskever, et al. 2014. Intrigu- ing properties of neural networks. In Proceedings of the International Conference on Learning Representations, ICLR, Banff, Canada.<br/><i><a href="#BIBREF5-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF50">Erman, L.D., F. Hayes-Roth, V.R. Lesser & D.R. Reddy. 1980. The Hearsay-II speech-understanding system: inte- grating knowledge to resolve uncertainty. ACM Comput. Surv. (CSUR). 12: 213-253.<br/><i><a href="#BIBREF50-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF51">Baars, B.J. & S. Franklin. 2003. How conscious experience and working memory interact. Trends. Cogn. Sci. 7: 166- 172.<br/><i><a href="#BIBREF51-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF52">Shanahan, M. 2006. A cognitive architecture that combines internal simulation with a global workspace. Conscious. Cogn. 15: 433-449.<br/><i><a href="#BIBREF52-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF53">Gilbert, C.D. & M. Sigman. 2007. Brain states: top-down influences in sensory processing. Neuron 54: 677-696.<br/><i><a href="#BIBREF53-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF54">Roelfsema, P.R. & F.P. de Lange. 2016. Early visual cortex as a multiscale cognitive blackboard. Annu. Rev. Vis. Sci. 2: 131-151.<br/><i><a href="#BIBREF54-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF55">Kahneman, D., A. Treisman & B.J. Gibbs. 1992. The review- ing of object files: object-specific integration of informa- tion. Cogn. Psychol. 24: 175-219.<br/><i><a href="#BIBREF55-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF56">Treisman, A. 1998. Feature binding, attention and object perception. Proc. R. Soc. 6: 1295-1306.<br/><i><a href="#BIBREF56-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF57">Nichols, E. & D. Hofstadter. 2009. Musicat: a model of music perception and expectation. In Proceedings of the 31st Annual Meeting of the Cognitive Science Society, Ams- terdam, The Netherlands.<br/><i><a href="#BIBREF57-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF58">Quinn, M.H., E. Conser, J.M. Witte & M. Mitchell. 2018. Semantic image retrieval via active grounding of visual sit- uations. In Proceedings of the International Conference on Semantic Computing, Laguna Hills, CA, 172-179.<br/><i><a href="#BIBREF58-Separation%20Of%20Representation%20And%20Mapping%20Processes.-ref">Separation Of Representation And Mapping Processes.</a></i></li>
                    
                        <li id="BIBREF59">Barnden, J.A. & K.J. Holyoak. 1994. Advances in Connec- tionist and Neural Computation Theory. Vol. 2. Analogical Connections. Norwood, NJ: Ablex Publishing Corporation.</li>
                    
                        <li id="BIBREF6">Eykholt, K., I. Evtimov, E. Fernandes, et al. 2018. Robust physical-world attacks on deep learning visual classifica- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, Salt Lake City, UT, 1625-1634.<br/><i><a href="#BIBREF6-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF60">Lu, H., Q. Liu, N. Ichien, et al. 2019. Seeing the meaning: vision meets semantics in solving pictorial analogy prob- lems. In Proceedings of the 41st Annual Meeting of the Cog- nitive Science Society, Montreal, Canada, 2201-2207.<br/><i><a href="#BIBREF60-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF60-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF61">Peyre, J., J. Sivic, I. Laptev & C. Schmid. 2019. Detect- ing unseen visual relations using analogies. In Proceedings of the IEEE International Conference on Computer Vision, ICCV, Seoul, South Korea, 1981-1990.<br/><i><a href="#BIBREF61-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF61-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF62">Hill, F., A. Santoro, D.G.T. Barrett, et al. 2019. Learning to make analogies by contrasting abstract relational structure. In Proceedings of the International Conference on Learning Representations, ICLR, New Orleans, LA.<br/><i><a href="#BIBREF62-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF63">Webb, T.W., Z. Dulberg, S.M. Frankland, et al. 2020. Learn- ing representations that support extrapolation. Preprint, arXiv:2007.05059.<br/><i><a href="#BIBREF63-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF64">Wang, K. & Z. Su. 2015. Automatic generation of Raven's progressive matrices. In Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI, Buenos Aires, Argentina, 903-909.<br/><i><a href="#BIBREF64-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF65">Carpenter, P.A., M.A. Just & P. Shell. 1990. What one intel- ligence test measures: a theoretical account of the process- ing in the Raven progressive matrices test. Psychol. Rev. 97: 404-431.<br/><i><a href="#BIBREF65-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF66">Barrett, D.G.T., F. Hill, A. Santoro, et al. 2018. Measuring abstract reasoning in neural networks. In Proceedings of the International Conference on Machine Learning, ICML, Stockholm, Sweden, 4477-4486.<br/><i><a href="#BIBREF66-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF67">Santoro, A., D. Raposo, D.G.T. Barrett, et al. 2017. A sim- ple neural network module for relational reasoning. In Advances in Neural Information Processing Systems, Long Beach, CA, 4968-4977.<br/><i><a href="#BIBREF67-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF68">Hahne, L., T. Lüddecke, F. Wörgötter & D. Kappel. 2019. Attention on abstract visual reasoning. Preprint, arXiv:1911.05990.<br/><i><a href="#BIBREF68-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF69">Jahrens, M. & T. Martinetz. 2020. Solving Raven's progres- sive matrices with multi-layer relation networks. In Pro- ceedings of the World Congress on Computational Intelli- gence, WCCI, Glasgow, Scotland.<br/><i><a href="#BIBREF69-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF7">Carey, S. 2011. Précis of The Origin of Concepts. Behav. Brain. Sci. 34: 113-124.<br/><i><a href="#BIBREF7-Introduction-ref">Introduction</a>, <a href="#BIBREF7-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF70">Steenbrugge, X., T. Verbelen, S. Leroux, & B. Dhoedt. 2018. Improving generalization for abstract reasoning tasks using disentangled feature representations. In NeurIPS2018, Part of 32nd Conference on Neural Information Processing Sys- tems, Montreal, Canada, 1-8.<br/><i><a href="#BIBREF70-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF71">Zheng, K., Z.-J. Zha & W. Wei. 2019. Abstract reasoning with distracting features. In Advances in Neural Informa- tion Processing Systems 32, Vancouver, BC, Canada, 5842- 5853.<br/><i><a href="#BIBREF71-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF72">Zhang, C., F. Gao, B. Jia, et al. 2019. RAVEN: a dataset for relational and analogical visual reasoning. In Proceed- ings of the IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR, Long Beach, CA, Vol. 2019, 5312- 5322.<br/><i><a href="#BIBREF72-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF73">He, K., X. Zhang, S. Ren & J. Sun. 2016. Deep residual learn- ing for image recognition. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, CVPR, Las Vegas, NV, 770-778.</li>
                    
                        <li id="BIBREF74">Zhuo, T. & M. Kankanhalli. 2020. Solving Raven's progressive matrices with neural networks. Preprint, arXiv:2002.01646.<br/><i><a href="#BIBREF74-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF75">Zhang, C., B. Jia, F. Gao, et al. 2019. Learning perceptual inference by contrasting. In Advances in Neural Informa- tion Processing Systems 32, Vancouver, BC, Canada, 1075- 1087.<br/><i><a href="#BIBREF75-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF76">Hu, S., Y. Ma, X. Liu, et al. 2021. Stratified rule-aware network for ab stract visual reasoning. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2021), Vancouver, BC, Canada.<br/><i><a href="#BIBREF76-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF77">Wu, Y., H. Dong, R. Grosse & J. Ba. 2020. The scat- tering compositional learner: discovering objects, attributes, relationships in analogical reasoning. Preprint, arXiv:2007.04212.<br/><i><a href="#BIBREF77-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF78">Zhang, C., B. Jia, S. Zhu & Y. Zhu. 2021. Abstract spatial- temporal reasoning via probabilistic abduction and execu- tion. Preprint, arXiv:2103.14230.<br/><i><a href="#BIBREF78-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF79">Webb, T.W., I. Sinha & J.D. Cohen. 2021. Emergent sym- bols through binding in external memory. In Proceedings of the International Conference on Learning Representations, ICLR, Sydney, Australia.<br/><i><a href="#BIBREF79-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF8">Spelke, E.S. & K.D. Kinzler. 2007. Core knowledge. Dev. Sci. 10: 89-96.<br/><i><a href="#BIBREF8-Introduction-ref">Introduction</a>, <a href="#BIBREF8-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF80">Funke, C.M., J. Borowski, K. Stosio, et al. 2020. Five points to check when comparing visual percep- tion in humans and machines. Preprint, arXiv:2004. 09406.<br/><i><a href="#BIBREF80-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF81">Geirhos, R., J.H. Jacobsen, C. Michaelis, et al. 2020. Short- cut learning in deep neural networks. Nat. Mach. Intell. 2: 665-673.<br/><i><a href="#BIBREF81-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF81-Discussion:%20How%20To%20Make%20Progress%20In%20Ai%20On%20Abstraction%20And%20Analogy-ref">Discussion: How To Make Progress In Ai On Abstraction And Analogy</a></i></li>
                    
                        <li id="BIBREF82">Marasović, A. 2018. NLP's generalization prob- lem, and how researchers are tackling it. Gradient. <a href="https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing." target="_blank">https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing.</a><br/><i><a href="#BIBREF82-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF83">Wang, J.X. 2020. Meta-learning in natural and artificial intelligence. Preprint, arXiv:2011.13464.<br/><i><a href="#BIBREF83-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF84">Lake, B.M., R. Salakhutdinov & J.B. Tenenbaum. 2015. Human-level concept learning through probabilistic pro- gram induction. Science. 350: 1332-1338.<br/><i><a href="#BIBREF84-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a>, <a href="#BIBREF84-Probabilistic%20Program%20Induction-ref">Probabilistic Program Induction</a></i></li>
                    
                        <li id="BIBREF85">Finn, C., P. Abbeel & S. Levine. 2017. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the International Conference on Machine Learning, ICML, Sydney, Australia, 1126-1135.<br/><i><a href="#BIBREF85-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF86">Ren, M., E. Triantafillou, S. Ravi, et al. 2018. Meta-learning for semi-supervised few-shot classification. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, BC, Canada.<br/><i><a href="#BIBREF86-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF87">Santoro, A., S. Bartunov, M. Botvinick, et al. 2016. Meta- learning with memory-augmented neural networks. In International Conference on Machine Learning (ICML 2016), New York City, NY, 1842-1850.<br/><i><a href="#BIBREF87-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF88">Snell, J., K. Swersky & R. Zemel. 2017. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, Long Beach, CA, 4078- 4088.<br/><i><a href="#BIBREF88-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF89">Hill, F., O. Tieleman, T. von Glehn, et al. 2021. Grounded language learning fast and slow. In Proceedings of the Inter- national Conference on Learning Representations, ICLR, Sydney, Australia.<br/><i><a href="#BIBREF89-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF9">Nosofsky, R.M. 1986. Attention, similarity, and the identification-categorization relationship. J. Exp. Psychol. 115: 39-57.<br/><i><a href="#BIBREF9-Introduction-ref">Introduction</a></i></li>
                    
                        <li id="BIBREF90">Nie, W., Z. Yu, A.B. Mao, et al. 2020. Bongard-LOGO: a new benchmark for human-level concept learning and reason- ing. Preprint, arXiv:2010.00763.<br/><i><a href="#BIBREF90-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF91">Lampinen, A.K. & J.L. McClelland. 2020. Transforming task representations to perform novel tasks. Proc. Natl. Acad. Sci., 117: 32970-32981.<br/><i><a href="#BIBREF91-Deep%20Learning%20Approaches-ref">Deep Learning Approaches</a></i></li>
                    
                        <li id="BIBREF92">Lake, B.M., R. Salakhutdinov & J.B. Tenenbaum. 2015. Supplementary material for human-level concept learn- ing through probabilistic program induction. Science 350: 1332-1338.<br/><i><a href="#BIBREF92-Probabilistic%20Program%20Induction-ref">Probabilistic Program Induction</a></i></li>
                    
                        <li id="BIBREF93">Feinman, R. & B.M. Lake. 2021. Learning task-general representations with generative neuro-symbolic modeling. In Proceedings of the International Conference on Learning Representations (ICLR).<br/><i><a href="#BIBREF93-Probabilistic%20Program%20Induction-ref">Probabilistic Program Induction</a></i></li>
                    
                        <li id="BIBREF94">Depeweg, S., C.A. Rothkopf & F. Jäkel. 2018. Solving Bon- gard problems with a visual language and pragmatic rea- soning. Preprint, arXiv:1804.04452.<br/><i><a href="#BIBREF94-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a>, <a href="#BIBREF94-P(R%7CE,%20G)%20%E2%88%9D%20P(E%7CR)P(R%7CG).-ref">P(R|E, G) ∝ P(E|R)P(R|G).</a></i></li>
                    
                        <li id="BIBREF95">Foundalis, H.E. Index of Bongard problems. <a href="http://www.foundalis.com/res/bps/bpidx.htm." target="_blank">http://www.foundalis.com/res/bps/bpidx.htm.</a><br/><i><a href="#BIBREF95-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a></i></li>
                    
                        <li id="BIBREF96">Bongard, M.M. 1970. Pattern Recognition. New York: Spar- tan Books.<br/><i><a href="#BIBREF96-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a></i></li>
                    
                        <li id="BIBREF97">Saito, K. & R. Nakano. 1995. A concept learning algorithm with adaptive search. In Machine Intelligence 14: Applied Machine Intelligence. K. Furukawa, D. Michie & S. Muggle- ton, Eds.: 347-363. Oxford: Clarendon Press.<br/><i><a href="#BIBREF97-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a></i></li>
                    
                        <li id="BIBREF98">Foundalis, H.E. 2006. PHAEACO: a cognitive architecture inspired by Bongard's problems. PhD thesis. Indiana Uni- versity, Bloomington, IN.<br/><i><a href="#BIBREF98-Solving%20Bongard%20Problems-ref">Solving Bongard Problems</a></i></li>
                    
                        <li id="BIBREF99">Chen, X., C. Liu & D. Song. 2019. Execution-guided neural program synthesis. In Proceedings of the International Con- ference on Learning Representations, ICLR, New Orleans, LA.<br/><i><a href="#BIBREF99-Summary%20Of%20Program-Induction%20Approaches-ref">Summary Of Program-Induction Approaches</a></i></li>
                    
                </ul>
            
        </footer>
    </div>
</article>
</main>


        <footer class="app__footer">



        </footer>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="a11y.js"></script>
        
        <div id="customContextMenu" style="display: none; position: absolute; z-index: 1000; background-color: #f0f0f0; padding: 10px; border: 1px solid #d4d4d4;">
            <button id="highlightText">Highlight</button>
            <button id="addComment">Add Comment</button>
            <button id="addSticker">Add Sticker</button>
        </div>
        
        <div id="dynamicContentArea"></div>


    </body>
</html>